{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2fb53f5-b55f-40f1-b8f6-460d16cd15dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab 3.2: Chunking Strategies for RAG\n",
    "\n",
    "Este notebook implementa diferentes estrat√©gias de chunking para os GCN Circulars,\n",
    "seguindo as melhores pr√°ticas dos labs oficiais da Databricks.\n",
    "\n",
    "**Objetivos:**\n",
    "1. Implementar chunking por caracteres (simples)\n",
    "2. Implementar chunking por senten√ßas (regex - compat√≠vel com serverless)\n",
    "3. Implementar chunking sem√¢ntico (par√°grafos)\n",
    "4. **Comparar estrat√©gias COM vs SEM overlap** e avaliar trade-offs\n",
    "5. **Analisar impacto do tamanho do chunk** na qualidade do retrieval\n",
    "6. Aplicar chunking ao dataset e salvar em Delta Lake\n",
    "\n",
    "**Exam Topics Covered:**\n",
    "- Section 2: Data Preparation (14%)\n",
    "  - Apply chunking strategy for document structure and model constraints\n",
    "  - Design retrieval systems using advanced chunking strategies\n",
    "  - Evaluate how chunk size and overlap affect retrieval precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1b2d88a-2240-4445-aa6a-6c222998aa92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d3462cc-0d6f-4148-bc20-38084f90e3e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instalar depend√™ncias"
    }
   },
   "outputs": [],
   "source": [
    "%pip install tiktoken -q\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "088417fe-4df0-4565-8dc4-10aeb7f67cd0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports e configura√ß√£o"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any\n",
    "from pyspark.sql.functions import (\n",
    "    col, udf, explode, lit, length,\n",
    "    monotonically_increasing_id, concat_ws, size, array\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Configura√ß√£o\n",
    "CATALOG = \"sandbox\"\n",
    "SCHEMA = \"nasa_gcn_dev\"\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")\n",
    "\n",
    "print(\"‚úÖ Setup completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54dc9c5d-fa55-4bfa-ac48-ffd44b318f25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Carregar dados preparados"
    }
   },
   "outputs": [],
   "source": [
    "# Carregar dataset do notebook anterior\n",
    "df_prepared = spark.table(\"gcn_circulars_prepared\")\n",
    "\n",
    "# Estat√≠sticas\n",
    "total_docs = df_prepared.count()\n",
    "avg_chars = df_prepared.agg({\"char_count\": \"avg\"}).collect()[0][0]\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä Dataset carregado:\n",
    "  - Total documentos: {total_docs:,}\n",
    "  - M√©dia de caracteres: {avg_chars:,.0f}\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar exemplo\n",
    "df_prepared.select(\"circular_id\", \"event_id\", \"char_count\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b1cc9ae-11a7-411e-b355-977cac069002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Estrat√©gia 1: Character-based Chunking\n",
    "\n",
    "Chunking simples por n√∫mero de caracteres com overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18368b10-c4e4-4f7a-9bc8-5b48dd08fd7a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definir fun√ß√£o de chunking por caracteres"
    }
   },
   "outputs": [],
   "source": [
    "def chunk_by_chars(text: str, chunk_size: int = 500, overlap: int = 100) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Divide texto em chunks de tamanho fixo com overlap.\n",
    "\n",
    "    Args:\n",
    "        text: Texto a ser dividido\n",
    "        chunk_size: Tamanho m√°ximo de cada chunk em caracteres\n",
    "        overlap: N√∫mero de caracteres de sobreposi√ß√£o entre chunks\n",
    "\n",
    "    Returns:\n",
    "        Lista de dicts com chunk_text, chunk_index, start_pos, end_pos\n",
    "    \"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "\n",
    "        # Tentar quebrar em espa√ßo para n√£o cortar palavras\n",
    "        if end < len(text):\n",
    "            # Procurar √∫ltimo espa√ßo dentro do chunk\n",
    "            last_space = text.rfind(' ', start, end)\n",
    "            if last_space > start:\n",
    "                end = last_space\n",
    "\n",
    "        chunk_text = text[start:end].strip()\n",
    "\n",
    "        if chunk_text:\n",
    "            chunks.append({\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"chunk_index\": chunk_idx,\n",
    "                \"start_pos\": start,\n",
    "                \"end_pos\": end,\n",
    "                \"char_count\": len(chunk_text)\n",
    "            })\n",
    "            chunk_idx += 1\n",
    "\n",
    "        # Pr√≥ximo chunk come√ßa com overlap\n",
    "        start = end - overlap if end < len(text) else len(text)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Testar\n",
    "test_text = \"This is a test. \" * 50\n",
    "test_chunks = chunk_by_chars(test_text, chunk_size=200, overlap=50)\n",
    "print(f\"Texto de {len(test_text)} chars ‚Üí {len(test_chunks)} chunks\")\n",
    "print(f\"Primeiro chunk: '{test_chunks[0]['chunk_text'][:50]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e321870-8935-441d-be33-4bba69aa6e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Estrat√©gia 2: Sentence-based Chunking (Regex)\n",
    "\n",
    "Chunking inteligente que respeita limites de senten√ßas.\n",
    "\n",
    "> **Nota:** Usamos regex ao inv√©s de NLTK para compatibilidade com clusters serverless.\n",
    "> NLTK requer download de dados que n√£o ficam dispon√≠veis nos workers do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72144f14-f98e-4ce1-b431-76fca8edda53",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definir fun√ß√£o de chunking por senten√ßas"
    }
   },
   "outputs": [],
   "source": [
    "# Nota: Usamos regex ao inv√©s de NLTK para compatibilidade com clusters serverless\n",
    "# NLTK requer download de dados que n√£o ficam dispon√≠veis nos workers\n",
    "\n",
    "def simple_sent_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenizador de senten√ßas simples usando regex.\n",
    "    Compat√≠vel com clusters serverless (n√£o requer NLTK).\n",
    "\n",
    "    Funciona bem para textos cient√≠ficos em ingl√™s como GCN Circulars.\n",
    "    \"\"\"\n",
    "    # Proteger abrevia√ß√µes comuns substituindo temporariamente\n",
    "    abbreviations = ['Dr.', 'Mr.', 'Mrs.', 'Ms.', 'Prof.', 'Fig.', 'Tab.', 'Eq.', 'et al.', 'i.e.', 'e.g.', 'vs.', 'etc.']\n",
    "    protected = text\n",
    "    for i, abbr in enumerate(abbreviations):\n",
    "        protected = protected.replace(abbr, f\"__ABBR{i}__\")\n",
    "\n",
    "    # Dividir em senten√ßas: . ! ? seguido de espa√ßo e letra mai√∫scula\n",
    "    sentences = re.split(r'([.!?]) +(?=[A-Z])', protected)\n",
    "\n",
    "    # Recombinar pontua√ß√£o com a senten√ßa anterior\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        if i + 1 < len(sentences) and sentences[i + 1] in '.!?':\n",
    "            result.append(sentences[i] + sentences[i + 1])\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(sentences[i])\n",
    "            i += 1\n",
    "\n",
    "    # Restaurar abrevia√ß√µes\n",
    "    final = []\n",
    "    for sent in result:\n",
    "        restored = sent\n",
    "        for i, abbr in enumerate(abbreviations):\n",
    "            restored = restored.replace(f\"__ABBR{i}__\", abbr)\n",
    "        if restored.strip():\n",
    "            final.append(restored.strip())\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "def chunk_by_sentences(text: str, max_chunk_size: int = 500, overlap_sentences: int = 1) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Divide texto em chunks respeitando limites de senten√ßas.\n",
    "\n",
    "    Args:\n",
    "        text: Texto a ser dividido\n",
    "        max_chunk_size: Tamanho m√°ximo aproximado de cada chunk\n",
    "        overlap_sentences: N√∫mero de senten√ßas de overlap\n",
    "\n",
    "    Returns:\n",
    "        Lista de chunks\n",
    "    \"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return []\n",
    "\n",
    "    # Tokenizar em senten√ßas (usando regex, compat√≠vel com serverless)\n",
    "    sentences = simple_sent_tokenize(text)\n",
    "\n",
    "    if len(sentences) == 0:\n",
    "        return [{\"chunk_text\": text, \"chunk_index\": 0, \"sentence_count\": 1, \"char_count\": len(text)}]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        # Se adicionar esta senten√ßa ultrapassa o limite\n",
    "        if current_size + sentence_size > max_chunk_size and current_chunk:\n",
    "            # Salvar chunk atual\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append({\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"chunk_index\": chunk_idx,\n",
    "                \"sentence_count\": len(current_chunk),\n",
    "                \"char_count\": len(chunk_text)\n",
    "            })\n",
    "            chunk_idx += 1\n",
    "\n",
    "            # Overlap: manter √∫ltimas N senten√ßas\n",
    "            current_chunk = current_chunk[-overlap_sentences:] if overlap_sentences > 0 else []\n",
    "            current_size = sum(len(s) for s in current_chunk)\n",
    "\n",
    "        current_chunk.append(sentence)\n",
    "        current_size += sentence_size\n",
    "\n",
    "    # √öltimo chunk\n",
    "    if current_chunk:\n",
    "        chunk_text = ' '.join(current_chunk)\n",
    "        chunks.append({\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"chunk_index\": chunk_idx,\n",
    "            \"sentence_count\": len(current_chunk),\n",
    "            \"char_count\": len(chunk_text)\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Testar\n",
    "test_text = \"GRB 251208B was detected by Fermi GBM. The burst had a duration of 2.5 seconds. \" * 20\n",
    "test_chunks = chunk_by_sentences(test_text, max_chunk_size=300, overlap_sentences=1)\n",
    "print(f\"Texto de {len(test_text)} chars ‚Üí {len(test_chunks)} chunks\")\n",
    "for i, c in enumerate(test_chunks[:2]):\n",
    "    print(f\"  Chunk {i}: {c['sentence_count']} senten√ßas, {c['char_count']} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fa986c2-a68f-452b-a0df-81828e214fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Estrat√©gia 3: Semantic/Paragraph Chunking\n",
    "\n",
    "Chunking que respeita par√°grafos e estrutura do documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf9a5dd0-bd90-4d64-8fe8-2b3b600eca86",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definir fun√ß√£o de chunking por par√°grafos"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def chunk_by_paragraphs(text: str, max_chunk_size: int = 800, min_paragraph_size: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Divide texto em chunks respeitando par√°grafos.\n",
    "\n",
    "    Args:\n",
    "        text: Texto a ser dividido\n",
    "        max_chunk_size: Tamanho m√°ximo de cada chunk\n",
    "        min_paragraph_size: Tamanho m√≠nimo para considerar um par√°grafo separado\n",
    "\n",
    "    Returns:\n",
    "        Lista de chunks\n",
    "    \"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return []\n",
    "\n",
    "    # Dividir por linhas duplas (par√°grafos)\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "\n",
    "    if len(paragraphs) == 0:\n",
    "        return [{\"chunk_text\": text, \"chunk_index\": 0, \"paragraph_count\": 1, \"char_count\": len(text)}]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para_size = len(para)\n",
    "\n",
    "        # Par√°grafo muito pequeno? Juntar com o anterior\n",
    "        if para_size < min_paragraph_size and current_chunk:\n",
    "            current_chunk.append(para)\n",
    "            current_size += para_size\n",
    "            continue\n",
    "\n",
    "        # Se adicionar ultrapassa o limite\n",
    "        if current_size + para_size > max_chunk_size and current_chunk:\n",
    "            chunk_text = '\\n\\n'.join(current_chunk)\n",
    "            chunks.append({\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"chunk_index\": chunk_idx,\n",
    "                \"paragraph_count\": len(current_chunk),\n",
    "                \"char_count\": len(chunk_text)\n",
    "            })\n",
    "            chunk_idx += 1\n",
    "            current_chunk = []\n",
    "            current_size = 0\n",
    "\n",
    "        current_chunk.append(para)\n",
    "        current_size += para_size\n",
    "\n",
    "    # √öltimo chunk\n",
    "    if current_chunk:\n",
    "        chunk_text = '\\n\\n'.join(current_chunk)\n",
    "        chunks.append({\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"chunk_index\": chunk_idx,\n",
    "            \"paragraph_count\": len(current_chunk),\n",
    "            \"char_count\": len(chunk_text)\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Testar\n",
    "test_text = \"\"\"GRB 251208B was detected by Fermi GBM on December 8, 2025 at 14:32:15 UT.\n",
    "\n",
    "The burst showed a complex light curve with multiple peaks. The T90 duration was measured at 2.5 seconds, classifying it as a short GRB.\n",
    "\n",
    "Follow-up observations were conducted with Swift XRT starting at T+300 seconds. An X-ray afterglow was detected at coordinates RA=123.456, Dec=-45.678.\n",
    "\n",
    "Optical observations from NOT revealed a fading counterpart with magnitude r=21.5 at T+2 hours.\"\"\"\n",
    "\n",
    "test_chunks = chunk_by_paragraphs(test_text, max_chunk_size=400)\n",
    "print(f\"Texto de {len(test_text)} chars ‚Üí {len(test_chunks)} chunks\")\n",
    "for i, c in enumerate(test_chunks):\n",
    "    print(f\"  Chunk {i}: {c['paragraph_count']} par√°grafos, {c['char_count']} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7139f9b0-aa89-4b3c-b4c8-aab7cd83ba18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Aplicar Chunking ao Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2162f24-0c0f-468a-9c79-5b9a1d95ef76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definir UDF para chunking"
    }
   },
   "outputs": [],
   "source": [
    "# Schema para os chunks\n",
    "chunk_schema = ArrayType(StructType([\n",
    "    StructField(\"chunk_text\", StringType(), True),\n",
    "    StructField(\"chunk_index\", IntegerType(), True),\n",
    "    StructField(\"char_count\", IntegerType(), True)\n",
    "]))\n",
    "\n",
    "# UDF para chunking por senten√ßas (melhor para textos cient√≠ficos)\n",
    "# IMPORTANTE: Todo o c√≥digo deve estar inline no UDF para funcionar em clusters serverless\n",
    "@udf(returnType=chunk_schema)\n",
    "def sentence_chunk_udf(text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    UDF que aplica chunking por senten√ßas.\n",
    "    C√≥digo inline para compatibilidade com serverless.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from typing import List, Dict, Any\n",
    "\n",
    "    def _sent_tokenize(text: str) -> List[str]:\n",
    "        \"\"\"Tokenizador de senten√ßas usando regex.\"\"\"\n",
    "        # Proteger abrevia√ß√µes comuns\n",
    "        abbreviations = ['Dr.', 'Mr.', 'Mrs.', 'Ms.', 'Prof.', 'Fig.', 'Tab.', 'Eq.', 'et al.', 'i.e.', 'e.g.', 'vs.', 'etc.']\n",
    "        protected = text\n",
    "        for i, abbr in enumerate(abbreviations):\n",
    "            protected = protected.replace(abbr, f\"__ABBR{i}__\")\n",
    "\n",
    "        # Dividir em senten√ßas\n",
    "        sentences = re.split(r'([.!?]) +(?=[A-Z])', protected)\n",
    "\n",
    "        # Recombinar pontua√ß√£o\n",
    "        result = []\n",
    "        i = 0\n",
    "        while i < len(sentences):\n",
    "            if i + 1 < len(sentences) and sentences[i + 1] in '.!?':\n",
    "                result.append(sentences[i] + sentences[i + 1])\n",
    "                i += 2\n",
    "            else:\n",
    "                result.append(sentences[i])\n",
    "                i += 1\n",
    "\n",
    "        # Restaurar abrevia√ß√µes\n",
    "        final = []\n",
    "        for sent in result:\n",
    "            restored = sent\n",
    "            for i, abbr in enumerate(abbreviations):\n",
    "                restored = restored.replace(f\"__ABBR{i}__\", abbr)\n",
    "            if restored.strip():\n",
    "                final.append(restored.strip())\n",
    "\n",
    "        return final\n",
    "\n",
    "    def _chunk_by_sentences(text: str, max_chunk_size: int = 500, overlap_sentences: int = 1) -> List[Dict[str, Any]]:\n",
    "        if not text or len(text) == 0:\n",
    "            return []\n",
    "\n",
    "        sentences = _sent_tokenize(text)\n",
    "\n",
    "        if len(sentences) == 0:\n",
    "            return [{\"chunk_text\": text, \"chunk_index\": 0, \"sentence_count\": 1, \"char_count\": len(text)}]\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        chunk_idx = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_size = len(sentence)\n",
    "\n",
    "            if current_size + sentence_size > max_chunk_size and current_chunk:\n",
    "                chunk_text = ' '.join(current_chunk)\n",
    "                chunks.append({\n",
    "                    \"chunk_text\": chunk_text,\n",
    "                    \"chunk_index\": chunk_idx,\n",
    "                    \"sentence_count\": len(current_chunk),\n",
    "                    \"char_count\": len(chunk_text)\n",
    "                })\n",
    "                chunk_idx += 1\n",
    "                current_chunk = current_chunk[-overlap_sentences:] if overlap_sentences > 0 else []\n",
    "                current_size = sum(len(s) for s in current_chunk)\n",
    "\n",
    "            current_chunk.append(sentence)\n",
    "            current_size += sentence_size\n",
    "\n",
    "        if current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append({\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"chunk_index\": chunk_idx,\n",
    "                \"sentence_count\": len(current_chunk),\n",
    "                \"char_count\": len(chunk_text)\n",
    "            })\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    # Executar chunking\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = _chunk_by_sentences(text, max_chunk_size=500, overlap_sentences=1)\n",
    "    return [{\"chunk_text\": c[\"chunk_text\"], \"chunk_index\": c[\"chunk_index\"], \"char_count\": c[\"char_count\"]}\n",
    "            for c in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "736055d3-cfbc-4072-9376-ffc109cb8b38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aplicar chunking"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import posexplode\n",
    "\n",
    "# Aplicar chunking\n",
    "df_chunked = df_prepared.withColumn(\n",
    "    \"chunks\", sentence_chunk_udf(col(\"body\"))\n",
    ")\n",
    "\n",
    "# Explodir chunks em linhas separadas\n",
    "df_exploded = df_chunked.select(\n",
    "    col(\"circular_id\"),\n",
    "    col(\"event_id\"),\n",
    "    col(\"subject\"),\n",
    "    col(\"created_on\"),\n",
    "    posexplode(col(\"chunks\")).alias(\"chunk_index\", \"chunk\")\n",
    ").select(\n",
    "    col(\"circular_id\"),\n",
    "    col(\"event_id\"),\n",
    "    col(\"subject\"),\n",
    "    col(\"created_on\"),\n",
    "    col(\"chunk_index\"),\n",
    "    col(\"chunk.chunk_text\").alias(\"chunk_text\"),\n",
    "    col(\"chunk.char_count\").alias(\"chunk_char_count\")\n",
    ")\n",
    "\n",
    "# Adicionar ID √∫nico para cada chunk\n",
    "df_final = df_exploded.withColumn(\n",
    "    \"chunk_id\",\n",
    "    concat_ws(\"_\", col(\"circular_id\").cast(\"string\"), col(\"chunk_index\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "# Mostrar resultado\n",
    "print(\"üìÑ Resultado do chunking:\")\n",
    "df_final.select(\"chunk_id\", \"circular_id\", \"event_id\", \"chunk_index\", \"chunk_char_count\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f12a20a-2689-4d6c-80c6-b6823e170cf8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Estat√≠sticas de chunking"
    }
   },
   "outputs": [],
   "source": [
    "# Estat√≠sticas\n",
    "chunk_stats = df_final.agg({\n",
    "    \"*\": \"count\",\n",
    "    \"chunk_char_count\": \"avg\",\n",
    "    \"chunk_char_count\": \"min\",\n",
    "    \"chunk_char_count\": \"max\"\n",
    "}).collect()[0]\n",
    "\n",
    "chunks_per_doc = df_final.groupBy(\"circular_id\").count()\n",
    "avg_chunks = chunks_per_doc.agg({\"count\": \"avg\"}).collect()[0][0]\n",
    "max_chunks = chunks_per_doc.agg({\"count\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä Estat√≠sticas de Chunking:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Total de chunks:           {df_final.count():,}\n",
    "Documentos originais:      {df_prepared.count():,}\n",
    "M√©dia de chunks/doc:       {avg_chunks:.1f}\n",
    "M√°ximo de chunks em 1 doc: {max_chunks}\n",
    "\n",
    "Tamanho dos chunks:\n",
    "  - M√≠nimo: {df_final.agg({'chunk_char_count': 'min'}).collect()[0][0]:,} chars\n",
    "  - M√©dio:  {df_final.agg({'chunk_char_count': 'avg'}).collect()[0][0]:,.0f} chars\n",
    "  - M√°ximo: {df_final.agg({'chunk_char_count': 'max'}).collect()[0][0]:,} chars\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f5c3757-a144-4165-ba63-fe19c8e80d0c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Distribui√ß√£o de tamanho dos chunks"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Categorizar chunks por tamanho\n",
    "df_size_dist = df_final.withColumn(\n",
    "    \"size_bucket\",\n",
    "    when(col(\"chunk_char_count\") < 200, \"tiny (<200)\")\n",
    "    .when(col(\"chunk_char_count\") < 400, \"small (200-400)\")\n",
    "    .when(col(\"chunk_char_count\") < 600, \"medium (400-600)\")\n",
    "    .when(col(\"chunk_char_count\") < 800, \"large (600-800)\")\n",
    "    .otherwise(\"very_large (800+)\")\n",
    ")\n",
    "\n",
    "print(\"üìè Distribui√ß√£o de tamanho dos chunks:\")\n",
    "df_size_dist.groupBy(\"size_bucket\").count().orderBy(\"size_bucket\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ed00192-0825-43c8-84df-d25ca462e2bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Salvar Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2f5d621-91ed-4440-a33b-67cf53010aad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Salvar tabela de chunks"
    }
   },
   "outputs": [],
   "source": [
    "# Criar documento formatado para embedding (incluindo metadados)\n",
    "df_to_save = df_final.withColumn(\n",
    "    \"document_for_embedding\",\n",
    "    concat_ws(\n",
    "        \"\\n\",\n",
    "        concat_ws(\": \", lit(\"EVENT\"), col(\"event_id\")),\n",
    "        concat_ws(\": \", lit(\"SUBJECT\"), col(\"subject\")),\n",
    "        lit(\"---\"),\n",
    "        col(\"chunk_text\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Salvar\n",
    "TABLE_NAME = \"gcn_circulars_chunks\"\n",
    "\n",
    "df_to_save.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(TABLE_NAME)\n",
    "\n",
    "saved_count = spark.table(TABLE_NAME).count()\n",
    "print(f\"‚úÖ Tabela {CATALOG}.{SCHEMA}.{TABLE_NAME} criada com {saved_count:,} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "239169c6-8ef6-4c5e-8031-72c7e49c88f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verificar tabela salva"
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar exemplos\n",
    "print(\"üìÑ Exemplos de chunks salvos:\")\n",
    "spark.table(TABLE_NAME).select(\n",
    "    \"chunk_id\", \"event_id\", \"chunk_index\", \"chunk_char_count\", \"document_for_embedding\"\n",
    ").show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8357f856-6883-4686-b1b4-9deb53839005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Estimativa de Tokens (para custo de embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc7c305d-8582-4bf9-af46-17089113cba2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calcular tokens estimados"
    }
   },
   "outputs": [],
   "source": [
    "# Usar tiktoken para estimativa mais precisa\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # Encoding usado pelo OpenAI/BGE\n",
    "\n",
    "# Sample para estimativa\n",
    "sample_chunks = spark.table(TABLE_NAME).select(\"chunk_text\").limit(1000).collect()\n",
    "total_tokens = sum(len(encoding.encode(row.chunk_text)) for row in sample_chunks)\n",
    "avg_tokens = total_tokens / len(sample_chunks)\n",
    "\n",
    "total_chunks = spark.table(TABLE_NAME).count()\n",
    "estimated_total_tokens = avg_tokens * total_chunks\n",
    "\n",
    "print(f\"\"\"\n",
    "üéØ Estimativa de Tokens:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Sample size:              {len(sample_chunks):,} chunks\n",
    "M√©dia tokens/chunk:       {avg_tokens:.0f}\n",
    "Total de chunks:          {total_chunks:,}\n",
    "Tokens estimados (total): {estimated_total_tokens:,.0f}\n",
    "\n",
    "üí∞ Custo estimado de embedding (databricks-bge-large-en):\n",
    "   ~$0.0001 por 1K tokens\n",
    "   Custo estimado: ${estimated_total_tokens/1000 * 0.0001:.2f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "078ce5d2-d16d-4eb4-9b2f-4d52fbf5254d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Comparar Estrat√©gias: Com vs Sem Overlap\n",
    "\n",
    "Uma das decis√µes mais importantes no chunking √© o **overlap** (sobreposi√ß√£o).\n",
    "Vamos comparar os resultados para entender o impacto.\n",
    "\n",
    "### üéØ Por que Overlap Importa?\n",
    "\n",
    "**Sem overlap:**\n",
    "```\n",
    "Chunk 1: \"...the burst was detected at T0.\"\n",
    "Chunk 2: \"Follow-up observations started at T+300s.\"\n",
    "```\n",
    "Uma query sobre \"when did observations start after detection\" pode perder o contexto.\n",
    "\n",
    "**Com overlap:**\n",
    "```\n",
    "Chunk 1: \"...the burst was detected at T0. Follow-up observations...\"\n",
    "Chunk 2: \"...detected at T0. Follow-up observations started at T+300s.\"\n",
    "```\n",
    "Ambos os chunks cont√™m o contexto completo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "221c7a8e-64cf-4687-88c0-fab6c222586d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Criar chunks SEM overlap para compara√ß√£o"
    }
   },
   "outputs": [],
   "source": [
    "def chunk_by_sentences_no_overlap(text: str, max_chunk_size: int = 500) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Chunking por senten√ßas SEM overlap.\"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return []\n",
    "\n",
    "    sentences = simple_sent_tokenize(text)\n",
    "    if len(sentences) == 0:\n",
    "        return [{\"chunk_text\": text, \"chunk_index\": 0, \"char_count\": len(text)}]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_size = len(sentence)\n",
    "\n",
    "        if current_size + sentence_size > max_chunk_size and current_chunk:\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunks.append({\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"chunk_index\": chunk_idx,\n",
    "                \"char_count\": len(chunk_text)\n",
    "            })\n",
    "            chunk_idx += 1\n",
    "            current_chunk = []  # SEM overlap\n",
    "            current_size = 0\n",
    "\n",
    "        current_chunk.append(sentence)\n",
    "        current_size += sentence_size\n",
    "\n",
    "    if current_chunk:\n",
    "        chunk_text = ' '.join(current_chunk)\n",
    "        chunks.append({\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"chunk_index\": chunk_idx,\n",
    "            \"char_count\": len(chunk_text)\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Comparar com um documento de exemplo\n",
    "sample_doc = df_prepared.select(\"body\").limit(1).collect()[0][0]\n",
    "\n",
    "chunks_with_overlap = chunk_by_sentences(sample_doc, max_chunk_size=500, overlap_sentences=1)\n",
    "chunks_no_overlap = chunk_by_sentences_no_overlap(sample_doc, max_chunk_size=500)\n",
    "\n",
    "print(f\"\"\"\n",
    "üî¨ Compara√ß√£o de Overlap:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Documento original: {len(sample_doc):,} caracteres\n",
    "\n",
    "COM overlap (1 senten√ßa):\n",
    "  - Total chunks: {len(chunks_with_overlap)}\n",
    "  - Chars total: {sum(c['char_count'] for c in chunks_with_overlap):,}\n",
    "  - Redund√¢ncia: {sum(c['char_count'] for c in chunks_with_overlap) - len(sample_doc):,} chars extras\n",
    "\n",
    "SEM overlap:\n",
    "  - Total chunks: {len(chunks_no_overlap)}\n",
    "  - Chars total: {sum(c['char_count'] for c in chunks_no_overlap):,}\n",
    "  - Redund√¢ncia: ~0 chars\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afcdcd70-2b78-4a88-a168-dc809a131b13",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Visualizar diferen√ßa de overlap"
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar as bordas dos chunks para ver o overlap\n",
    "print(\"üìã Primeiros 3 chunks COM overlap:\")\n",
    "print(\"=\" * 80)\n",
    "for i, chunk in enumerate(chunks_with_overlap[:3]):\n",
    "    print(f\"\\nChunk {i} ({chunk['char_count']} chars):\")\n",
    "    # Mostrar in√≠cio e fim\n",
    "    text = chunk['chunk_text']\n",
    "    print(f\"  In√≠cio: '{text[:80]}...'\")\n",
    "    print(f\"  Fim:    '...{text[-80:]}'\")\n",
    "\n",
    "print(\"\\n\\nüìã Primeiros 3 chunks SEM overlap:\")\n",
    "print(\"=\" * 80)\n",
    "for i, chunk in enumerate(chunks_no_overlap[:3]):\n",
    "    print(f\"\\nChunk {i} ({chunk['char_count']} chars):\")\n",
    "    text = chunk['chunk_text']\n",
    "    print(f\"  In√≠cio: '{text[:80]}...'\")\n",
    "    print(f\"  Fim:    '...{text[-80:]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37ad3d80-c946-45d1-8af9-dbade75ac8ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Avaliar Impacto do Tamanho do Chunk\n",
    "\n",
    "O tamanho do chunk afeta diretamente a qualidade do retrieval:\n",
    "\n",
    "| Tamanho | Pr√≥s | Contras |\n",
    "|---------|------|---------|\n",
    "| **Pequeno** (100-200 chars) | Alta precis√£o, foco | Pode perder contexto |\n",
    "| **M√©dio** (300-500 chars) | Bom equil√≠brio | Escolha mais comum |\n",
    "| **Grande** (600-1000 chars) | Mais contexto | Menor precis√£o, dilui relev√¢ncia |\n",
    "\n",
    "### üéØ Recomenda√ß√µes por Caso de Uso:\n",
    "\n",
    "- **FAQ / Perguntas diretas**: Chunks menores (200-300)\n",
    "- **Documentos t√©cnicos**: Chunks m√©dios (400-600)\n",
    "- **Artigos cient√≠ficos**: Chunks maiores (600-800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b555580-d9e0-4b89-8f79-9b616758bc39",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Testar diferentes tamanhos de chunk"
    }
   },
   "outputs": [],
   "source": [
    "chunk_sizes = [200, 400, 600, 800]\n",
    "results = []\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    chunks = chunk_by_sentences(sample_doc, max_chunk_size=size, overlap_sentences=1)\n",
    "    total_chars = sum(c['char_count'] for c in chunks)\n",
    "    avg_chars = total_chars / len(chunks) if chunks else 0\n",
    "\n",
    "    results.append({\n",
    "        \"max_size\": size,\n",
    "        \"num_chunks\": len(chunks),\n",
    "        \"avg_chunk_size\": avg_chars,\n",
    "        \"total_chars\": total_chars,\n",
    "        \"overhead_pct\": ((total_chars - len(sample_doc)) / len(sample_doc)) * 100\n",
    "    })\n",
    "\n",
    "print(\"üìä Impacto do Tamanho do Chunk:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Max Size':<12} {'Chunks':<10} {'Avg Size':<12} {'Total':<12} {'Overhead %':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for r in results:\n",
    "    print(f\"{r['max_size']:<12} {r['num_chunks']:<10} {r['avg_chunk_size']:<12.0f} {r['total_chars']:<12,} {r['overhead_pct']:<12.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "898f94ec-f8f0-4bd1-bc39-2b66a076c157",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "An√°lise de trade-offs"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üìà An√°lise de Trade-offs:\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "1. CHUNKS PEQUENOS (200-300 chars):\n",
    "   ‚úÖ Alta precis√£o no retrieval\n",
    "   ‚úÖ Menos tokens por chunk = menor custo de LLM\n",
    "   ‚ùå Pode fragmentar informa√ß√£o relacionada\n",
    "   ‚ùå Mais chunks = mais chamadas de embedding\n",
    "\n",
    "2. CHUNKS M√âDIOS (400-600 chars):\n",
    "   ‚úÖ Bom equil√≠brio entre precis√£o e contexto\n",
    "   ‚úÖ Adequado para a maioria dos casos\n",
    "   ‚úÖ Tamanho t√≠pico de 1-3 senten√ßas completas\n",
    "   ‚Üí RECOMENDADO para GCN Circulars\n",
    "\n",
    "3. CHUNKS GRANDES (800+ chars):\n",
    "   ‚úÖ Contexto rico para respostas complexas\n",
    "   ‚ùå Menor precis√£o (dilui relev√¢ncia)\n",
    "   ‚ùå Mais tokens = maior custo de LLM\n",
    "   ‚ùå Pode incluir informa√ß√£o irrelevante\n",
    "\n",
    "üí° Nossa Escolha: 500 chars com overlap de 1 senten√ßa\n",
    "   - Preserva contexto cient√≠fico\n",
    "   - Compat√≠vel com limite de tokens do embedding model\n",
    "   - Overlap garante continuidade sem√¢ntica\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca3bb20a-9e44-4ea7-9387-930247d2ff89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Conceitos-Chave para o Exame\n",
    "\n",
    "### üìö Chunking Strategies (Section 2: Data Preparation - 14%)\n",
    "\n",
    "| Estrat√©gia | Quando Usar | Trade-off |\n",
    "|------------|-------------|-----------|\n",
    "| **Fixed-size** | Documentos uniformes | Simples, mas pode cortar contexto |\n",
    "| **Sentence-based** | Textos em prosa | Preserva sem√¢ntica, overhead moderado |\n",
    "| **Paragraph-based** | Docs estruturados | Respeita estrutura, chunks vari√°veis |\n",
    "| **Semantic** | Docs complexos | Melhor qualidade, mais complexo |\n",
    "\n",
    "### üéØ Exam Tips:\n",
    "\n",
    "1. **Overlap** previne perda de contexto nas bordas dos chunks\n",
    "2. **Chunk size** deve considerar:\n",
    "   - Limite de tokens do embedding model (tipicamente 512)\n",
    "   - Janela de contexto do LLM\n",
    "   - Custo de embedding e infer√™ncia\n",
    "3. **Metadata enrichment** melhora retrieval (source, date, section)\n",
    "4. **Delta Lake** √© preferido para armazenar chunks (ACID, versioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c8159d-f311-4ae5-a406-64f2bc6ced7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Resumo das decis√µes de chunking"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üìã Resumo: Decis√µes de Chunking para GCN Circulars\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Par√¢metro           ‚îÇ Valor Escolhido                        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Estrat√©gia          ‚îÇ Sentence-based (regex)                 ‚îÇ\n",
    "‚îÇ Max chunk size      ‚îÇ 500 caracteres (~125 tokens)           ‚îÇ\n",
    "‚îÇ Overlap             ‚îÇ 1 senten√ßa                             ‚îÇ\n",
    "‚îÇ Min doc size        ‚îÇ 100 caracteres (filtrado antes)        ‚îÇ\n",
    "‚îÇ Metadata inclu√≠do   ‚îÇ event_id, subject, created_on          ‚îÇ\n",
    "‚îÇ Storage             ‚îÇ Delta Lake (Unity Catalog)             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Justificativas:\n",
    "1. Sentence-based preserva contexto cient√≠fico dos GCN Circulars\n",
    "2. 500 chars √© compat√≠vel com embeddings BGE (max 512 tokens)\n",
    "3. Overlap de 1 senten√ßa previne perda de contexto\n",
    "4. Regex usado ao inv√©s de NLTK para compatibilidade serverless\n",
    "5. Metadata enriquece retrieval com informa√ß√µes do evento\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f370c9b-e64a-420b-9b64-f11c6cfb1388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Lab Wrap-Up: Key Learnings\n",
    "\n",
    "### ‚úÖ O que voc√™ aprendeu:\n",
    "\n",
    "| Etapa | Conceito | Aplica√ß√£o |\n",
    "|-------|----------|-----------|\n",
    "| **Chunking por caracteres** | Divis√£o simples com overlap | Baseline, documentos uniformes |\n",
    "| **Chunking por senten√ßas** | Respeita limites sem√¢nticos | Textos cient√≠ficos, prosa |\n",
    "| **Chunking por par√°grafos** | Preserva estrutura do documento | Docs com se√ß√µes claras |\n",
    "| **Compara√ß√£o de overlap** | Trade-off redund√¢ncia vs contexto | Decis√£o de design |\n",
    "| **An√°lise de chunk size** | Impacto em precis√£o e custo | Otimiza√ß√£o |\n",
    "\n",
    "### üß† Insights Cr√≠ticos:\n",
    "\n",
    "1. **Qualidade > Quantidade**: Chunks bem estruturados superam volume\n",
    "2. **Overlap √© essencial**: Previne perda de contexto em bordas\n",
    "3. **Tamanho importa**: Muito pequeno fragmenta, muito grande dilui\n",
    "4. **Metadata enriquece**: Source, date, section melhoram retrieval\n",
    "5. **Serverless requer adapta√ß√£o**: NLTK n√£o funciona, regex sim\n",
    "\n",
    "### üöÄ Pr√≥ximos Passos:\n",
    "\n",
    "1. **Embeddings**: Gerar vetores com BGE model\n",
    "2. **Vector Search**: Criar √≠ndice para retrieval\n",
    "3. **RAG Chain**: Conectar retriever ao LLM\n",
    "4. **Avalia√ß√£o**: Medir qualidade do retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6da1da60-0cf6-4677-a3bb-c6f615434e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pr√≥ximos Passos\n",
    "\n",
    "‚úÖ Chunks criados e salvos\n",
    "‚û°Ô∏è Pr√≥ximo notebook: `03-embeddings-vector-search.py`\n",
    "   - Gerar embeddings com modelo BGE\n",
    "   - Criar √≠ndice Vector Search\n",
    "   - Testar retrieval"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-chunking",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
