# Databricks Certified Generative AI Engineer Associate
## Complete Study Guide

---

# Part 1: Foundations and Design

## Chapter 1: Exam Overview and Preparation

### Exam Structure

| Item | Value |
|------|-------|
| Questions | 45 multiple choice |
| Time | 90 minutes |
| Fee | $200 USD |
| Passing | ~70% |
| Validity | 2 years |

### Exam Sections and Weights

| Section | Weight | Key Topics |
|---------|--------|------------|
| Design Applications | ~14% | Prompt engineering, Compound AI systems |
| Data Preparation | ~14% | RAG, Chunking, Document parsing |
| Application Development | ~30% | Vector search, Agents, LangChain |
| Assembling and Deploying | ~22% | MLflow, Model Serving |
| Governance | ~8% | Security, Guardrails |
| Evaluation and Monitoring | ~12% | Metrics, LLM-as-Judge, LLMOps |

### Study Tips

1. **Focus on Application Development (30%)** - Largest exam weight
2. **Master MLflow** - Appears across multiple sections
3. **Practice chunking strategies** - Essential for RAG
4. **Understand guardrails** - Key for governance
5. **Know ai_query()** - Important for batch inference

---

## Chapter 2: Prompt Engineering Fundamentals

### What is Prompt Engineering?

**Prompt**: An AI prompt is an input or query given to a large language model to elicit a specific response or output.

**Prompt Engineering**: The practice of designing and refining prompts to optimize the responses generated by AI models. Similar to explaining a concept differently depending on the listener's background, the clarity and context within a prompt determine how well the model understands and responds.

### Components of a Good Prompt

1. **Instruction**: A clear directive specifying what the model should do
2. **Context**: Background information providing necessary details to understand the task
3. **Input/Question**: The specific query or data the model needs to process
4. **Output Type/Format**: The desired structure or style of the response

### Prompting Techniques

#### Zero-shot Prompting
Generates text or performs a task **without providing any examples**. The model relies solely on the instruction provided.

#### Few-shot Prompting
Provides **a few input-output examples** to guide the model. Especially useful for tasks like sentiment analysis where examples can guide the model's response.

#### Prompt Chaining
Multiple tasks linked together, with the output of one prompt serving as the input for the next. Useful for breaking complex tasks into manageable steps.

#### Chain-of-Thought (CoT) Prompting
Enhances reasoning by guiding the model to **articulate thought processes step-by-step**. Useful for logical reasoning and math problems.

### Prompt Engineering Tips

- **Prompts are model-specific**: Different models may require different prompts
- **Use delimiters**: Distinguish between instruction and context using `###`, backticks, braces, or dashes
- **Request structured output**: Ask for HTML, JSON, tables, or markdown
- **Guide the model**:
  - "Do not make things up if you do not know"
  - "Do not make assumptions based on nationalities"
  - "Explain how you solve this step-by-step"

### Benefits and Limitations

**Benefits:**
- Simple and efficient
- Predictable results
- Tailored outputs

**Limitations:**
- Output depends on the model used
- Limited by pre-trained model's internal knowledge
- **For external knowledge, we need RAG**

---

## Chapter 3: Designing Generative AI Applications

### Crafting Prompts for Structured Output

LLMs offer flexibility, but without precise instructions, they often generate inconsistent, verbose, or unpredictable responses. For enterprise applications—such as legal audits, financial summaries, or customer support workflows—you need model outputs that follow a specific structure, such as JSON, SQL, or tabular formats.

**Example - Vague Prompt:**
```
Summarize this support ticket
```

**Example - Structured Prompt:**
```
Summarize the following customer support ticket and return the result in JSON format, including the fields customer_id, issue_summary, and urgency_level.
```

### Strategies for Structured Prompting

#### Using Delimiters and Schemas
*Delimiters* are clear markers that separate instructions from input. *Schemas* define the structure of the desired output.

Example: Wrap input in `<input>...</input>` and specify output format.

#### Including Few-shot Examples
*Few-shot* prompting involves providing 1–2 examples within the prompt to illustrate the expected behavior.

### Matching Model Tasks to Use Cases

#### Common LLM Task Types

| Task Type | Description | Example Use Case |
|-----------|-------------|-----------------|
| **Text Generation** | Open-ended content creation | Product descriptions, emails |
| **Classification** | Assigning categories/labels | Email routing, sentiment analysis |
| **Extraction** | Pulling structured fields from text | ICD-10 codes, invoice data |
| **Transformation** | Changing format or style | Query to SQL, legalese to plain English |

### Task Type Decision Matrix

| Use Case | Desired Output | Recommended Task | Prompt Format |
|----------|---------------|------------------|---------------|
| Support ticket routing | Category label | Classification | Classify this ticket as: billing, technical, or general |
| Financial KPI extraction | JSON object | Extraction | Extract revenue, expenses, and margin in JSON |
| Legal clause simplification | Rewritten text | Transformation | Rewrite this clause in plain English |
| Product description | Natural language | Text Generation | Write a brief product description |

### Best Practices for Prompt-Task Matching

1. **Start with the desired output format** - Work backward from what downstream systems expect
2. **Clarify downstream consumers** - Know who/what will use the output
3. **Differentiate between open-ended and closed-ended tasks**
4. **Make your intent explicit** - The model should never have to guess
5. **Iterate and test with edge cases**

---

# Part 2: RAG and Data Preparation

## Chapter 4: Introduction to RAG (Retrieval Augmented Generation)

### How do Language Models Learn Knowledge?

| Approach | Description | Requirements |
|----------|-------------|--------------|
| **Pre-Training** | Training from scratch | Billions to trillions of tokens |
| **Fine-Tuning** | Adapting to specific domains | Thousands of domain-specific examples |
| **Passing Context (RAG)** | Combining LLM with external retrieval | External knowledge base |

RAG is the **least complex** and **most compute-efficient** approach.

### What is RAG?

**Retrieval Augmented Generation (RAG)** is a pattern that improves LLM applications by leveraging custom data. It works by:
1. **Retrieving** relevant documents based on a query
2. **Augmenting** the prompt with retrieved context
3. **Generating** an improved response

**Key Benefit**: Solves the **knowledge gap** problem - models trained on outdated data can access current information.

### RAG Workflow Components

| Component | Function |
|-----------|----------|
| **Index & Embed** | Create vector representations of documents and queries |
| **Vector Store** | Store unstructured data indexed by vectors |
| **Retrieval** | Search stored vectors using similarity search |
| **Filtering & Reranking** | Select/rank retrieved documents |
| **Prompt Augmentation** | Inject retrieved data into the prompt |
| **Generation** | LLM generates the final response |

### Benefits of RAG Architecture

1. **Up-to-date responses**: Not limited to static training data
2. **Reduced hallucinations**: Outputs include citations for verification
3. **Domain-specific contextualization**: Tailored to proprietary data
4. **Cost-effectiveness**: No fine-tuning overhead, easy to update

### RAG in Databricks

| RAG Step | Databricks Feature |
|----------|-------------------|
| Find Relevant Context | Mosaic AI Vector Search |
| Generate Response | Mosaic AI Model Serving (Foundation Models) |
| Serve RAG Chain | Mosaic AI Model Serving |

---

## Chapter 5: Preparing and Chunking Data for RAG

### Why Chunking Matters

Chunking involves dividing large documents into smaller, logically coherent sections. This is significant because LLMs cannot process entire documents at once; they operate within a **context window** that limits the amount of text they can comprehend.

Chunking requires considering how a human reader would break down and comprehend the content. For example:
- A legal contract might be best chunked by clauses
- A user manual might work better when chunked by sections or step-by-step instructions

### Chunking Strategies

#### Fixed-Length Chunking
- Divides documents into equal-sized blocks (tokens, characters, or words)
- **Best for**: Logs, structured data, CSV files
- **Pros**: Simple, fast
- **Cons**: May break context mid-sentence

#### Sentence-Level Chunking
- Breaks text into grammatically complete, sentence-sized pieces
- **Best for**: News articles, FAQs, knowledge base articles
- **Pros**: Preserves grammar
- **Cons**: May lose broader context

#### Paragraph-Based Chunking
- Each paragraph becomes a chunk
- **Best for**: Academic papers, reports, blogs
- **Pros**: Human-readable, preserves natural thought boundaries
- **Cons**: Inconsistent sizes

#### Sliding Window Chunking
- Creates overlapping chunks to preserve context across boundaries
- **Best for**: QA systems, legal documents
- **Pros**: Maintains continuity
- **Cons**: Resource-intensive, increases storage

#### Semantic Chunking
- Uses NLP tools to detect natural boundaries in meaning
- **Best for**: Books, transcripts, complex unstructured content
- **Pros**: Meaningful, adaptive chunks
- **Cons**: Complex to implement, requires embeddings

### Chunking Strategy Comparison

| Strategy | Best For | Pros | Cons |
|----------|----------|------|------|
| Fixed-Length | Logs, structured data | Simple, fast | May break context |
| Sentence-Level | Articles, FAQs | Preserves grammar | Limited broader context |
| Paragraph-Based | Reports, blogs | Human-readable | Inconsistent sizes |
| Sliding Window | QA systems, legal docs | Maintains continuity | Resource-intensive |
| Semantic | Books, transcripts | Meaningful, adaptive | Complex to implement |

### Controlling Overlap and Granularity

**Granularity** refers to the amount of content in each chunk (tokens, characters, or sentences).

- **Too large**: May include irrelevant or distracting content
- **Too small**: May exclude key information

**Fixed Overlap**: Consistently repeats a set number of tokens between chunks.

**Dynamic Overlap**: Uses content-aware rules to decide where chunks should begin and end.

### Impact of Chunk Size on Retrieval

| Chunk Size | Retrieval Relevance | Context Coverage | System Latency |
|------------|---------------------|------------------|----------------|
| Small | High | Low | High |
| Medium | Balanced | Balanced | Moderate |
| Large | Low | High | Low |

**Tip**: Start with medium-sized chunks (150–300 tokens) and adjust based on retrieval performance.

### Content Filtering and Extraction

#### Removing Redundancy and Noise
Common sources of noise:
- Web crawled documents with footers, cookie notices, ads
- PDFs with repeated headers and footnotes
- Corporate documents with embedded disclaimers
- OCR-scanned documents with garbled characters

#### Extracting Content from PDFs and Images
- **PyMuPDF/pdfminer.six**: Digital PDFs with standard formatting
- **Tesseract OCR**: Scanned or image-based documents
- **Adobe PDF Services API**: Complex or formatted PDFs

#### Converting to Delta Format
Delta Lake is the preferred storage layer in Databricks for RAG systems:
- **Query efficiency**: Predicate pushdown, indexing, caching
- **Incremental updates**: Update specific chunks without full rewrite
- **Auditability**: Transaction log, time travel
- **Compatibility**: Integrates with MLflow, Feature Store, Vector Search

---

## Chapter 6: Mosaic AI Vector Search

### What is Mosaic AI Vector Search?

A service that:
- Stores vector representations of data plus metadata
- Integrates tightly with the Lakehouse
- Provides scalable, low-latency production service
- Supports ACLs via Unity Catalog
- Offers REST API and Python client for similarity search

### Vector Search Methods

#### Method 1: Delta Sync API with Managed Embeddings
- **Automatic sync** from Delta Table
- **Fully managed embeddings** - no pre-calculation needed
- Specify table, column to embed, and model to use
- Same model used for queries ensures consistency

#### Method 2: Delta Sync API with Self-Managed Embeddings
- **Automatic sync** from Delta Table
- **Bring your own embeddings** - calculate using any model
- Store vectors in Delta table, then create index
- System keeps index in sync with source table

#### Method 3: Direct Access CRUD API
- **Manual sync** via API
- Full CRUD operations for specific records
- Useful for updating individual embeddings without rebuilding

### Setup Steps

1. **Create Vector Search Endpoint**: Compute resource that powers the system
2. **Create Model Serving Endpoint** (if using managed embeddings): Foundation Models, External Models, or Custom Models
3. **Create Vector Search Index**: Auto-synced from Delta table, governed by Unity Catalog

### Vector Search Fundamentals

**Core Components:**
- **Embedding model**: Converts content into numerical vectors
- **Index**: Structured, searchable storage for vectors
- **Query Interface**: API that accepts input embedding and returns similar vectors
- **Metadata filters**: Criteria to refine search results by attributes

**Retrieval Methods:**
- Cosine similarity
- Euclidean distance

---

# Part 3: Application Development

## Chapter 7: Compound AI Systems

### Shift from Models to Systems

To achieve production-grade generative AI applications, we need **Compound AI Systems** - not just individual models. These systems combine multiple components working together.

### Key Concepts

| Concept | Definition |
|---------|------------|
| **Intents** | What the user wants to accomplish |
| **Tasks** | Individual operations to fulfill intents |
| **Pipelines** | Connected chains of tasks |
| **Chains** | LLM-based sequences of operations |

### Decomposing Applications

1. Identify user intents
2. Break intents into tasks
3. Connect tasks into pipelines
4. Implement LLM-based chains where appropriate

---

## Chapter 8: Building GenAI Applications with LangChain

### LangChain Fundamentals

LangChain enables you to design and deploy generative AI applications that integrate prompts, chains, and agents with external tools, including databases, APIs, and search engines.

### Prompt Templates

A prompt template in LangChain defines a structure that takes variables and produces a formatted prompt string.

```python
from langchain.prompts import PromptTemplate

template = """You are a helpful assistant.
Summarize the following research abstract in three sentences:
{text}"""

prompt = PromptTemplate(
    input_variables=["text"],
    template=template
)
```

### Chains

A chain links one or more components together. In simplest form, a chain connects a prompt template with an LLM to process input and return an output.

```python
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain

llm = ChatOpenAI(model="gpt-4", temperature=0.3)
chain = LLMChain(llm=llm, prompt=prompt_template)
response = chain.run(topic="quantum computing")
```

### Memory and Context Management

#### ConversationBufferMemory
Stores the entire conversation history in plain text. Ideal for short conversations where full context is critical.

#### ConversationBufferWindowMemory
Keeps only the last `k` interactions, balancing context retention with efficiency in longer conversations.

#### ConversationKGMemory
Builds a knowledge graph of entities and relationships mentioned in the conversation. Supports structured reasoning in domains like healthcare, finance, or law.

### LangChain Agents

An **agent** in LangChain interprets a user's request, determines which tools or actions are needed, and executes them to generate a final response.

#### How Agents Work:
1. User submits input
2. Agent interprets query and determines needed tools
3. Agent calls tools and integrates results
4. Agent formulates final response

#### Agent Types:
- **Zero-shot agents**: Decide on tools without prior examples
- **Conversational agents**: Maintain context using memory

### Comparison: Chains vs Agents

| Feature | Chains | Agents |
|---------|--------|--------|
| Flow Control | Predefined and static | Dynamic, step-by-step decisions |
| Flexibility | Limited to defined structure | High, adapts to varied queries |
| Tool Integration | Explicit linking required | Runtime selection based on needs |
| Use Case | Predictable, repetitive tasks | Complex, variable tasks |

### Model and Tool Integration

#### Plugging in Tools
Tools allow your application to reach beyond the model's training data:
- **APIs**: Real-time external information
- **Databases**: Trusted internal data
- **Calculators**: Precise computations

```python
from langchain.tools import Tool
import requests

def get_weather(city: str):
    url = f"https://api.weatherapi.com/v1/current.json?key=API_KEY&q={city}"
    response = requests.get(url)
    return response.json()["current"]["condition"]["text"]

weather_tool = Tool(
    name="WeatherAPI",
    func=get_weather,
    description="Fetches the current weather for a given city"
)
```

### Retrieval Integration with Chains

Retrieval in LangChain typically involves three components:
1. **Document store**: Vector database (FAISS, Pinecone, Weaviate)
2. **Retriever**: Search mechanism over the document store
3. **Chain**: Pipeline that combines retriever output with model reasoning

```python
from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
response = qa_chain.run("What are the key benefits of Delta Lake?")
```

---

## Chapter 9: Agents and Cognitive Architectures

### Non-Agentic vs Agentic Workflows

**Non-Agentic:**
- Workflow stages are predefined
- Order of operations is hard-coded
- Deterministic execution

**Agentic:**
- Agent decides actions dynamically
- LLM reasons about what to do next
- Non-deterministic execution

### What is an Agent?

An application that executes complex tasks by using a language model to:
- Define a sequence of actions
- Reason on the task
- Choose tools dynamically

### Core Agent Components

| Component | Function |
|-----------|----------|
| **Task** | What we're asking it to do |
| **LLM (Brain)** | Reasoning engine that figures out how to complete the task |
| **Tools** | External systems (APIs, databases, search) |
| **Memory** | Track what's been done |
| **Planning** | Define how to execute tasks |

### Agent Reasoning Patterns

#### ReAct (Reason + Act)
A loop of:
1. **Thought**: Reflect on the problem
2. **Act**: Choose and use a tool
3. **Observe**: Evaluate results
4. **Repeat** until task is complete

#### Tool Use
Agents interact with external tools:
- Search engines
- Image generation
- Document retrieval
- Code execution
- APIs

#### Planning
Agents dynamically adjust goals based on:
- Changing conditions
- Observations during execution
- Complex sub-task orchestration

#### Multi-Agent Collaboration
Multiple specialized agents working together:
- Each agent focuses on specific tasks
- Improves performance through specialization
- Enables scaling to complex problems

### Agent Frameworks

| Framework | Description |
|-----------|-------------|
| **LangChain Agents** | Structure for building tool-using agents |
| **AutoGPT** | LLM that controls other LLMs |
| **OpenAI Function Calling** | Model calls defined functions |
| **AutoGen** | Multi-agent framework |
| **Hugging Face Agents** | Natural language API for transformers |

---

## Chapter 10: Multi-Stage Reasoning

### Why Multi-stage Reasoning?

Single prompts can become complex and error-prone. Breaking tasks into stages:
- Improves accuracy
- Enables validation at each step
- Makes debugging easier
- Allows component reuse

### Tool Ordering for Sequential Logic

**Tool ordering** is arranging tools or prompts in a logical sequence.

Example - Financial Assistant:
1. Search recent earnings PDFs using a document retriever
2. Extract financial data using a document parser or LLM
3. Summarize the extracted data using a language model

### Planning Agent Chains

**Planning agents** combine reasoning with tool selection to make dynamic decisions about what to do next, step by step.

```python
from langchain.agents import initialize_agent, Tool

tools = [
    Tool(name="SearchLogs", func=search_logs,
         description="Search system logs for outages."),
    Tool(name="WeatherChecker", func=check_weather,
         description="Check current weather conditions.")
]

agent = initialize_agent(
    tools=tools, llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)
```

### Multi-Agent System Architecture

Multiple agents assigned specialized roles collaborate to solve complex tasks:

1. **User input** → Coordinator agent for analysis
2. **Coordinator** evaluates query and determines which specialized agents needed
3. **Specialized agents** (compliance, data analysis, domain knowledge) process subtasks
4. **Result synthesis** - Coordinator gathers outputs and synthesizes final response

---

# Part 4: Deployment and MLflow

## Chapter 11: MLflow for RAG Applications

### MLflow in the RAG Workflow

MLflow supports:
- **Experiment Tracking**: Log parameters, metrics, artifacts
- **Model Flavors**: Package chains as models
- **Model Registry**: Version and manage chains

### Key MLflow Features

#### Model Tracking
- Log chain/model details
- Record LLM parameters (temperature, etc.)
- Attach evaluation artifacts
- Link to source code

#### Model Flavors
MLflow supports multiple "flavors":
- **LangChain**: Package entire chains
- **pyfunc**: Custom Python logic
- Classic ML (scikit-learn, XGBoost, PyTorch)

#### Model Registry
- Version chains
- Assign aliases (dev, staging, prod)
- Lineage tracking
- Full history visibility

### Assembling a RAG Application

1. User query → Embedding
2. Embedding → Vector Search (retrieval)
3. Retrieved docs → Prompt template
4. Prompt → LLM (generation)
5. Response → User

The chain acts as a lightweight **orchestrator**:
- Vector search runs on Mosaic AI
- Completions run on Model Serving

---

## Chapter 12: Model Deployment on Databricks

### Pyfunc Model Structure

A Pyfunc model in Databricks includes:
- **Artifacts**: Serialized files (vector indexes, tokenizers, prompt templates)
- **Conda/environment specification**: Dependencies and package versions
- **python_function flavor**: Implementation of `predict()` method

```python
import mlflow.pyfunc

class RAGModelWrapper(mlflow.pyfunc.PythonModel):
    def load_context(self, context):
        self.chain = load_langchain_pipeline(context.artifacts["rag_config"])

    def predict(self, context, model_input):
        return self.chain.run(model_input["query"])

mlflow.pyfunc.save_model(
    path="rag_pyfunc_model",
    python_model=RAGModelWrapper(),
    artifacts={"rag_config": "/dbfs/path/to/rag_config.json"},
    conda_env="conda.yaml"
)
```

### Deployment Options

| Type | Use Case | Databricks Feature |
|------|----------|-------------------|
| **Batch** | Process large datasets | ai_query() with Spark |
| **Real-time** | Interactive applications | Model Serving Endpoints |

### Local vs Hosted Deployment

| Criteria | Local Deployment | Hosted (Model Serving) |
|----------|-----------------|------------------------|
| Use Case | Development, debugging | Production workloads |
| Performance | Dependent on local resources | Auto-scaled |
| Cost | Low (local hardware) | Pay-per-compute |
| Security | Local network control | Built-in access control |
| Integration | Limited to local endpoints | REST API, enterprise systems |

### MLflow Tracking and Registration

```python
with mlflow.start_run() as run:
    mlflow.log_param("retriever.top_k", 5)
    mlflow.log_param("llm.temperature", 0.1)
    mlflow.log_metric("val_exact_match", 0.63)

    mlflow.pyfunc.save_model(
        path="rag_pyfunc_model",
        python_model=RAGModel(),
        signature=sig,
        artifacts={...}
    )
```

### Batch Deployment with ai_query()

```sql
SELECT id, query,
       ai_query(
         "databricks-model-serving-endpoint",
         CONCAT("Answer based on context: ", context_column, " Question: ", query)
       ) AS generated_answer
FROM user_queries;
```

### Real-time Deployment

Model Serving provides:
- Low-latency inference
- Auto-scaling
- High availability
- REST API endpoints

---

# Part 5: Evaluation and Governance

## Chapter 13: Evaluation Techniques

### LLM Evaluation vs Classical ML

| Aspect | Classical ML | LLMs |
|--------|-------------|------|
| **Data Requirements** | Moderate | Massive |
| **Metrics** | Clear (accuracy, F1) | Complex (language quality) |
| **Interpretability** | Easier | Black box |

### Base Model Metrics

| Metric | What it Measures |
|--------|-----------------|
| **Loss** | How well model predicts next token |
| **Perplexity** | Model confidence in word choice |
| **Toxicity** | Harmfulness of output |

### Task-Specific Metrics

#### BLEU (Bilingual Evaluation Understudy)
- Used for translation tasks
- Compares generated output to reference
- Measures n-gram overlap (unigrams, bigrams, trigrams)
- Higher scores = better matches

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
- Used for summarization
- Counts word overlap with reference
- Variants: ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum

### Retrieval Evaluation Metrics

#### Precision
Measures the proportion of retrieved chunks that are actually relevant.
```
Precision = (Relevant Chunks Retrieved) / (Total Chunks Retrieved)
```

#### Recall
Measures the proportion of all relevant chunks that were successfully retrieved.
```
Recall = (Relevant Chunks Retrieved) / (Total Relevant Chunks Available)
```

### Benchmarking

**Mosaic AI Gauntlet**: 35 benchmarks across 6 categories:
1. Reading comprehension
2. Commonsense reasoning
3. Problem solving
4. World knowledge
5. Symbolic problem solving
6. Language understanding

### LLM-as-a-Judge

When no reference datasets exist, use an LLM to evaluate outputs.

**Requirements:**
- Examples of good/bad outputs
- Clear scoring criteria
- Grading rubric

**Limitations:**
- Lack of contextual awareness
- Potential hallucinations
- Bias concerns

**Solution**: Human-in-the-loop review

### MLflow Evaluation

MLflow provides:
- **Batch evaluation** using LLM-as-a-Judge
- **Interactive evaluation** in UI
- **Custom metrics** support
- Cost-effective automated evaluation

**Enhanced Workflow:**
1. Create example evaluation records
2. Create a metric object with examples, scoring criteria, judge model
3. Evaluate model against reference dataset using custom metric

---

## Chapter 14: Quality and Safety Mechanisms

### Common Issues in Model Responses

#### Hallucination
The model produces content that may sound plausible but is factually incorrect or fabricated. Models generate responses by predicting the next most probable sequence of words rather than checking against factual databases.

#### Bias
Models trained on massive datasets inherit stereotypes or skewed perspectives embedded in that content. Training data inevitably contains human opinions, stereotypes, and cultural imbalances.

#### Toxicity
The model generates offensive, discriminatory, or harmful language. Toxicity arises when models reproduce offensive patterns from training data.

### Prompt Augmentation for Safety

**Techniques:**
1. **Role specification**: Clearly state the role the model should assume
2. **Context enrichment**: Provide background information or approved reference data
3. **Output constraints**: Define strict formatting or content rules
4. **Fallback instructions**: Tell the model how to respond if information is unavailable

```python
prompt_template = PromptTemplate(
    input_variables=["article"],
    template="""You are a factual and neutral summarizer.
Summarize the following article in three bullet points.
Avoid speculative language and include only verifiable details.
Article: {article}"""
)
```

### Implementing Guardrails

Guardrails are external mechanisms that validate, filter, or modify model responses before they reach the user.

**Techniques:**
- **Content filtering**: Detect and block harmful outputs
- **Response validation**: Check formats, ranges, compliance
- **External libraries**: Guardrails AI, Microsoft Presidio
- **Fallback handling**: Return safe message if rules violated

### Guardrails Comparison

| Scenario | Without Guardrails | With Guardrails |
|----------|-------------------|-----------------|
| Healthcare | Fabricates treatment details | Cites only approved guidelines |
| Finance | Speculative advice without compliance | Validates against SEC filings |
| Customer Support | Insensitive phrasing | Filters toxic language |

---

## Chapter 15: Security and Governance

### Key Security Concerns

1. **Prompt Injection**: Malicious inputs that manipulate model behavior
2. **Data Leakage**: Sensitive information in outputs
3. **Hallucinations**: False or misleading information
4. **Bias**: Unfair or discriminatory outputs
5. **Toxicity**: Harmful content generation

### Guardrails Implementation

| Guardrail Type | Purpose |
|----------------|---------|
| **Input Validation** | Filter malicious prompts |
| **Output Filtering** | Remove harmful content |
| **PII Detection** | Mask sensitive data |
| **Content Moderation** | Block inappropriate responses |

### Databricks Security Features

- **Unity Catalog**: Access control for data and models
- **Llama Guard**: Open-source content moderation
- **DASF Framework**: Databricks AI Security Framework

### Access and Version Control

| Role | Registry Permissions | Endpoint Permissions | Responsibility |
|------|---------------------|---------------------|----------------|
| MLOps Engineer | Transition versions | Manage configs | Promote/rollback |
| Data Scientist | Create runs, register models | Read-only | Improve quality |
| App Team | None | Query endpoint | Integrate responses |
| Security Admin | Manage ACLs | Manage ACLs | Enforce RBAC |

---

## Chapter 16: LLMOps and Monitoring

### LLMOps vs MLOps

LLMOps extends MLOps for LLM-specific needs:
- Prompt versioning
- Chain management
- Evaluation pipelines
- Cost optimization

### Monitoring Considerations

| Metric | Why Monitor |
|--------|-------------|
| **Latency** | User experience |
| **Cost** | Token usage and API calls |
| **Quality** | Response accuracy |
| **Drift** | Model performance over time |
| **Usage** | Patterns and anomalies |

### Databricks Monitoring Features

- **Inference Tables**: Log all requests/responses
- **Lakehouse Monitoring**: Drift detection
- **Agent Monitoring**: Track live endpoints

### Endpoint Optimization

| Bottleneck | Strategy | Latency Impact | Cost Impact |
|------------|----------|----------------|-------------|
| High vector retrieval time | Enable ANN indexing | ↓ Significant | Neutral |
| Inefficient embedding usage | Implement caching | ↓ Moderate | ↓ Moderate |
| Endpoint cold starts | Higher min_workers | ↓ High | ↑ Higher |
| Oversized indexes | Prune low-value vectors | Neutral | ↓ High |
| Spiky traffic | Enable autoscaling | ↓ High under load | ↑ Variable |

---

# Key Technologies Summary

| Technology | Use |
|------------|-----|
| **MLflow** | Tracking, Registry, Tracing, Evaluation |
| **Unity Catalog** | Governance, access control |
| **Mosaic AI Vector Search** | Semantic search, embeddings |
| **Model Serving** | Real-time inference, FM APIs |
| **LangChain** | Chains, agents, tools |
| **Delta Lake** | Data storage for RAG |

---

# Practice Questions

## Section 1: Design Applications

1. **What is the main purpose of using a `PromptTemplate` in LangChain?**
   - A. To speed up LLM inference
   - B. To control token length
   - C. To define reusable, parameterized prompts with consistent structure ✓
   - D. To deploy models to REST API

2. **Which use case requires the extraction task type?**
   - A. Generating a summary
   - B. Classifying customer emails
   - C. Pulling fields like revenue and expenses from a financial document ✓
   - D. Rewriting a legal clause

## Section 2: Data Preparation

3. **What is the primary reason to apply overlapping in chunking strategies?**
   - A. To reduce storage costs
   - B. To ensure chunks contain complete and continuous context ✓
   - C. To prevent duplication
   - D. To reduce chunks required

4. **Which is a drawback of fixed-length chunking?**
   - A. Computationally expensive
   - B. Preserves semantic coherence
   - C. Can break logical units of meaning, reducing retrieval accuracy ✓
   - D. Requires specialized model

## Section 3: Application Development

5. **Which memory type in LangChain maintains conversation context?**
   - A. Improves token efficiency
   - B. Enables models to recall and reuse prior conversation context ✓
   - C. Filters unsafe content
   - D. Connects models with SQL databases

6. **What advantage do agents provide over fixed chains?**
   - A. Require fewer resources
   - B. Execute only pre-defined prompts
   - C. Dynamically select tools based on user requests ✓
   - D. Remove need for retrieval augmentation

## Section 4: Deployment

7. **Which describes the role of a PyFunc model in RAG deployment?**
   - A. Provides graphical interface for vector indexing
   - B. Wraps pre-processing, inference, and post-processing into standardized predict function ✓
   - C. Automatically generates embeddings
   - D. Only used for logging metrics

8. **What is the primary benefit of metadata filters in Vector Search?**
   - A. Reduce embedding dimensionality
   - B. Allow restricting retrieval to specific attributes, improving relevance ✓
   - C. Automatically update indices
   - D. Increase token budget

## Section 5: Governance

9. **Which scenario illustrates hallucination in an LLM response?**
   - A. Model refuses to answer when uncertain
   - B. Model cites a non-existent medical study as evidence ✓
   - C. Model uses neutral language
   - D. Model filters profanity

10. **What is the main function of prompt augmentation for safety?**
    - A. Increase training data
    - B. Enrich prompts with roles, context, and rules for safer outputs ✓
    - C. Store conversation history
    - D. Reduce query cost

## Section 6: Evaluation

11. **Which metric measures how many retrieved chunks are actually relevant?**
    - A. Recall
    - B. Precision ✓
    - C. BLEU
    - D. Perplexity

12. **What is LLM-as-a-Judge used for?**
    - A. Training new models
    - B. Evaluating outputs when no reference datasets exist ✓
    - C. Reducing inference costs
    - D. Managing model versions

---

*Consolidated from Databricks Academy training materials and O'Reilly study guide*
