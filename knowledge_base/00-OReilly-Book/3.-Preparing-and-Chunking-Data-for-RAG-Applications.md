# Chapter 3. Preparing and Chunking Data for RAG Applications

Imagine you’re asking a large language model (LLM) to answer a question using a massive stack of documents. If those documents are disorganized, repetitive, or full of irrelevant details, the model may struggle to find the right answer, just as you would in a cluttered library. Retrieval-augmented generation (RAG) systems work similarly: the cleaner and more structured the data, the better the model performs. Rather than simply feed the model more content, the goal is to curate, filter, and format that content so it’s useful at the moment of retrieval. In this chapter, you’ll explore hands-on techniques for breaking up documents into manageable pieces (a process called _chunking_), cleaning noisy inputs, and structuring data for high-quality results. These are not just theoretical exercises—they’re essential skills for passing the Databricks Certified Generative AI Associate exam and building effective AI systems in the real world.

Preparing data for retrieval in RAG involves more than basic preprocessing. In this context, _noisy inputs_ refer to parts of a document that are irrelevant, redundant, or unstructured in a way that interferes with the model’s ability to retrieve useful information. Examples include repeated paragraphs, formatting artifacts, headers, and footers, as well as unrelated content such as advertisements in web-scraped data. Removing or correcting these elements ensures that only high-quality, relevant data is used during retrieval. You’ll need to strategically chunk documents, filter out redundant or noisy content, and convert the source material into structured formats suitable for efficient semantic search. These operations are foundational for building scalable and reliable AI systems, especially when working with enterprise data in Databricks environments.

# Prerequisites

Before starting this chapter, make sure you’re familiar with the following:

  * What LLMs are and how they work at a high level

  * The concept of model inference and common output types (e.g., text, structured data)

  * Basic Python syntax and Databricks notebooks


# Learning Objectives

After completing this chapter, you will be able to do all of the following:

  * Implement document chunking strategies that optimize retrieval performance in RAG workflows.

  * Apply filtering techniques to reduce noise and redundancy in source data.

  * Convert and structure content into Delta format for efficient querying.

  * Evaluate and tune retrieval quality using precision, recall, and reranking metrics.

  * Troubleshoot gaps in retrieval by diagnosing data preparation issues.


# Document Analysis and Chunking

To utilize RAG systems effectively, you must begin with properly structured input data. One of the most important early steps in this process is chunking, which involves dividing large documents into smaller, logically coherent sections. This division is significant because LLMs cannot process entire documents at once; they operate within a _context window_ that limits the amount of text they can comprehend. If the text isn’t chunked carefully, the model might miss key information or return irrelevant answers.

Chunking isn’t just a mechanical or formatting task. It requires you to consider how a human reader would break down and comprehend the content. For example, a legal contract might be best chunked by clauses, while a user manual might work better when chunked by sections or step-by-step instructions. These decisions affect what parts of the document are retrieved in response to a query—and, therefore, whether the model generates a useful answer.

In this section, you’ll learn how to analyze your source documents with an eye toward structure and purpose. You’ll explore various chunking strategies—from simple fixed-length blocks to more advanced techniques like semantic chunking that rely on natural language patterns. You’ll also see how tools like Python’s spaCy library and Databricks notebooks can help automate chunking while keeping it aligned with your specific use case.

By the end of this section, you’ll understand why chunking is not just a technical prerequisite but a strategic choice that can make or break the quality of a RAG-based application.

# Chunking Strategies and When to Use Them

When working with documents in a RAG pipeline, it’s essential to prepare them in a manner that allows a language model to comprehend them. One key step is breaking large documents into smaller, manageable sections—known as chunks. Language models have a limited capacity for processing text—the context window, as mentioned. Chunking ensures that each piece of the document is small enough to fit within that window while still carrying meaningful information.

Not all chunking is equal. There are various chunking strategies, each tailored to specific document types and use cases. Some prioritize simplicity, while others aim to preserve the natural flow or meaning of the content. Choosing the right approach requires understanding how your users will query the data, what kind of content you’re working with, and what the language model needs to generate accurate responses. This subsection introduces these chunking strategies, compares their strengths and weaknesses, and explains how to decide which to use for your specific retrieval goals.

To help you evaluate each method clearly, the following sections outline each major chunking strategy, along with its use case, trade-offs, and a real-world example.

## Fixed-Length Chunking

The simplest approach and one of the easiest to automate, fixed-length chunking divides documents into equal-sized blocks of uniform size based on a defined number of tokens, characters, or words. This method ignores sentence or paragraph boundaries, which means the chunks may start or end mid-sentence or mid-thought. 

Let’s examine where this strategy is best suited.

This method is particularly useful for documents that are consistently structured or where context is evenly distributed across the text, such as CSV files, log files, or records with repeating formats. For instance, if each entry in a server log contains roughly the same kind of information, fixed-length chunks can facilitate fast and predictable processing.

Here’s a real-world example: a cybersecurity team utilizes logs from firewall activity to train an RAG model, which helps analysts respond to potential threats. Since each log entry follows the same schema (for example, timestamp, source IP, destination IP, and action), breaking this data into fixed 50-line chunks enables the model to retrieve and examine relevant portions quickly. The uniform structure makes fixed-length chunking both reliable and scalable in this context.

But consider what you give up when choosing this strategy. While it’s simple and fast to implement, fixed-length chunking carries a high risk of breaking up logical units of meaning. A sentence might be split between two chunks, making it harder for the language model to understand the complete context. This ambiguity can lead to poor retrieval relevance and weaker generative responses, particularly in content that relies on full sentences or coherent paragraphs.

## Sentence-Level Chunking

Now let’s look at a more linguistically aware method: sentence-level chunking. This approach breaks text into grammatically complete, sentence-sized pieces for clearer meaning and easier parsing. Here, each chunk contains a fixed number of complete sentences (for example, 3–5 sentences per chunk). This method ensures that each chunk forms a grammatically complete unit, preserving sentence structure and improving the readability of the retrieved content. It’s especially helpful when working with text that already follows clear sentence formatting.

When is this approach a good fit? It works well for moderately structured content, such as news articles, FAQs, and internal knowledge base articles. These formats typically present information in short, standalone sections where individual sentences convey useful meaning without relying heavily on long-range context.

Imagine a company’s customer support team maintaining a large internal FAQ document that covers common troubleshooting steps. When a support agent queries a RAG system for guidance on how to handle a login issue, sentence-level chunking enables the model to retrieve focused, standalone advice, such as _Reset your password using the account recovery link_ , without mixing in unrelated steps. Since each chunk contains a few complete and self-contained answers, retrieval remains precise and on-topic.

There are limitations to this method. It does maintain grammatical integrity and avoids fragmenting sentences, which helps the model interpret each chunk more accurately. However, it may lose broader context if a concept spans multiple sentences across chunks. For example, multi-step instructions could be split in a way that leaves each chunk incomplete without the others.

## Paragraph-Based Chunking

Next, we have paragraph-based chunking—a method that aligns naturally with the structure of many documents. In this approach, each _paragraph_ becomes a chunk, assuming the document has clear paragraph separation. This strategy preserves the natural boundaries of thought that paragraphs represent, making it easier for the language model to interpret the information within context.

This strategy is ideal for well-formatted text such as academic papers, reports, and blogs. These document types often use paragraphs to separate ideas, methods, or arguments, which makes them excellent candidates for paragraph-based chunking.

A real-world example might involve a data science team using technical whitepapers to train a RAG-based assistant for internal use. Each whitepaper contains detailed sections on methodology, results, and analysis, organized by paragraph. By chunking the document at the paragraph level, the assistant can retrieve a specific explanation of a model’s architecture or a performance result without dragging in unrelated content. This keeps the response precise and tied to one conceptual unit.

Of course, there are downsides.**** Paragraph-based chunking is easy to implement and often produces human-readable results. But the sizes of paragraphs can vary greatly, leading to inconsistency in chunk length. Short paragraphs may lack sufficient context, while long ones could exceed the model’s token limit, reducing the effectiveness of retrieval and generation.

## Sliding Window Chunking

The sliding window chunking approach is a strategy that prioritizes continuity by intentionally overlapping content. Here, you create overlapping chunks to preserve context across boundaries. For example, a 500-token chunk with a 100-token overlap allows each new chunk to retain some of the context from the previous chunk. This method ensures that information split across chunk boundaries isn’t lost during retrieval.

Sliding window is effective for question-answering systems or any scenario where ideas span across multiple sentences or paragraphs. It is particularly helpful in legal or technical documents where precision and continuity are critical.

A real-world example could involve a legal tech company building a RAG system to help users understand contract clauses. Legal language often spans multiple paragraphs and relies heavily on cross-references and precise phrasing. Using sliding window chunking, the system ensures that parts of a clause or definition that might be split at a boundary still appear in adjacent chunks. This preserves the meaning and increases the likelihood that the correct legal interpretation is retrieved.

Be aware of the trade-offs.**** Although sliding window chunking improves context retention and retrieval accuracy, it also increases the number of chunks stored and processed. This results in higher computational and storage costs, particularly for large-scale document repositories.

## Semantic Chunking

Now we turn to the most advanced technique. Semantic chunking adapts to the content’s meaning rather than relying on size or surface structure.

This advanced technique uses natural language processing (NLP) tools to detect natural boundaries in meaning—splitting at topic changes, heading levels, or discourse markers. Unlike simpler methods that rely on length or fixed structure, semantic chunking tries to understand what the text is about before deciding where to split it.

Semantic chunking is best for long-form unstructured content like transcripts, meeting notes, or books. These types of documents often contain shifts in topic or speaker that don’t align with clean sentence or paragraph boundaries.

Consider a real-world scenario: a healthcare organization wants to use doctor-patient visit transcripts in a RAG application to help administrative staff extract key insights. These conversations often shift topics, starting with symptoms, then moving to history, and finally to treatment. Using semantic chunking, the system can segment the transcript into meaningful sections based on topic changes rather than sentence count. This allows the model to retrieve only the section related to medication recommendations, for example, without including unrelated small talk or examination details.

One tradeoff is that semantic chunking requires more computation and preprocessing, often involving embeddings and similarity scoring to detect semantic shifts. While this increases complexity and processing time, the resulting chunks tend to be the most contextually rich and useful for retrieval tasks involving complex, nuanced information.

###### Note

Semantic chunking often uses embeddings and similarity scores to detect topic shifts. Tools like spaCy or transformers from Hugging Face can help.

[Table 3-1](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_table_1_1755111238471949) compares the different shunking strategies with their pros and cons.

Table 3-1. Chunking strategies, pros and consStrategy| Best For| Pros| Cons  
---|---|---|---  
Fixed-Length| Logs, structured data| Simple, fast| May break context  
Sentence-Level| Articles, FAQs| Preserves grammar| Limited broader context  
Paragraph-Based| Reports, blogs| Human-readable, easy to chunk| Inconsistent sizes  
Sliding Window| QA systems, legal documents| Maintains continuity| Resource-intensive  
Semantic| Books, transcripts, complex docs| Meaningful, adaptive| Complex to implement  
  
In practice, you’ll often combine chunking techniques. For example, you might use paragraph-based chunking followed by a sliding window to handle variability in paragraph length. The key is to balance retrieval granularity with enough context for the LLM to generate a coherent response. For example, you could implement two of these strategies using open-source NLP libraries to create retrievable chunks from raw text.

## Controlling Overlap and Granularity

When preparing documents for RAG, one of the most important design decisions involves how to divide your content into chunks. This involves two key parameters: _granularity_ , which refers to the size of each chunk, and _overlap_ , which describes whether adjacent chunks share some of the same content.

These decisions directly impact how well your system retrieves relevant information and how effectively an LLM can comprehend that information in context. For example, if your chunks are too large, they may contain extra, unrelated content that distracts the model. If too small, they may lack enough context for the model to generate a useful response.

Overlap becomes important when the meaning of the text spans across chunk boundaries. Without overlap, you might cut a sentence or paragraph in the middle, causing the model to lose track of the full idea. Adding some repeated content between chunks can help preserve continuity, though it also increases processing overhead and may introduce redundancy.

The trick is finding the right balance between chunk size and overlap based on the structure of your documents and the needs of your RAG system. These techniques are especially useful when working with documents such as technical manuals, research papers, or lengthy articles, where information is interconnected and cannot be easily separated.

Let’s begin by learning what granularity means in the context of chunking and how different levels affect retrieval. _Granularity_ refers to the amount of content included in each chunk, typically measured in tokens, characters, or sentences. In the context of a RAG system, selecting the right chunk size is crucial because it determines the amount of context the model receives when answering a question. If the chunk is too large, it might include irrelevant or distracting content. If small, key pieces of information might be excluded, reducing the model’s ability to generate a useful response.

The ideal chunk size often depends on the structure, length, and density of the content:

  * A product FAQ typically consists of brief, self-contained answers where each question and answer form a discrete, meaningful unit. This structure makes it a great candidate for fine-grained chunking, such as by individual sentences or question-answer pairs. By keeping each chunk small and focused, you ensure that the model receives highly relevant and unambiguous input during retrieval. For instance, if a user asks a question about refund policies, retrieving a single FAQ chunk specifically answering that topic is more efficient and accurate than retrieving an entire document section. Smaller chunks, like these, also reduce noise and enhance the relevance of matches during similarity search, making them especially valuable in customer support and knowledge base applications.

  * A legal contract, on the other hand, is composed of dense, interdependent clauses that are often legally binding and contextually linked. Each clause may refer to previous or subsequent sections, which means that splitting these arbitrarily could cause the language model to miss or misinterpret critical information. In such cases, using paragraph-based chunking helps preserve the integrity of the legal logic and terminology.


If you were to chunk a contract by individual sentences or tokens, a clause defining “termination conditions” might be separated from related clauses about “notice period” or “penalties.” This fragmentation would hurt both retrieval and comprehension. By chunking at the paragraph level, you capture complete legal ideas in a single unit, making it more likely that a retrieval system will return all necessary context for downstream analysis or generation.

Legal documents are a prime example of where _coarse_ granularity—larger chunk sizes—is not only acceptable but necessary. The downside is that such chunks may approach or exceed model token limits, so careful evaluation and possible truncation strategies should be considered during implementation.

It’s also important to consider the model’s token limit when designing your chunks. Most LLMs, including those used in Databricks, have a maximum context window, typically ranging from 4,000 to 8,000 tokens, depending on the model version. If a chunk exceeds that limit, it may be truncated or rejected altogether, which can degrade the quality of the model’s responses.

At the same time, if your chunks are too small, they may not include enough information to answer the user’s query meaningfully. This results in poor retrieval accuracy and less helpful outcomes. 

###### Tip

The goal is to strike a balance: keep chunks concise enough to fit within the model’s input limits, but rich enough in context to be useful.

When planning your chunking strategy, consider the total _context budget_ the model has to work with and aim to maximize the relevance of each token included. [Example 3-1](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_example_1_1755111238477444) shows a basic example that splits a text into token-based chunks.

##### Example 3-1. Controlling chunk granularity using token-based splitting
    
    
     def tokenize_and_chunk(text, chunk_size=200):
        """
        Splits a given text into non-overlapping chunks based on token count.
        This example uses a fixed granularity of 200 tokens per chunk.
        """
        import tiktoken  # Tokenizer compatible with OpenAI models
     
        enc = tiktoken.get_encoding("cl100k_base")
        tokens = enc.encode(text)
     
        chunks = []
        for i in range(0, len(tokens), chunk_size):
            chunk = tokens[i:i + chunk_size]
            if chunk:
                chunks.append(enc.decode(chunk))
     
        return chunks

###### Tip

Use a larger `chunk_size` (for example, 300–500 tokens) for documents that contain interrelated content like contracts or policies. Use smaller chunks (100–200 tokens) for short, distinct facts or frequently asked questions (FAQs). Start with chunks that fall in the 100–300 token range, then test and adjust based on retrieval performance and your application’s needs. 

## Overlapping Chunks for Contextual Flow: Fixed and Dynamic

_Overlap_ means that each chunk shares a portion of its content with the previous or next chunk. This technique is particularly useful in scenarios where information does not fall neatly into independent, isolated blocks. Many real-world texts—like academic papers, technical documentation, or product manuals—present ideas that unfold over multiple sentences or paragraphs. If you split this type of content into strictly non-overlapping chunks, the flow of information can get interrupted, leading to poorer comprehension by the model.

For example, imagine you are chunking a user manual for a complex software system. If one chunk ends with a partial description of a command and the next chunk starts with its usage example, the language model may miss the connection if the command itself wasn’t repeated. By overlapping the final sentence or two of the first chunk with the start of the second, you preserve continuity and improve the model’s ability to follow the logic.

Overlap helps ensure that each chunk is semantically complete—that is, that it contains enough self-contained meaning to make sense on its own, even if retrieved in isolation. This is especially important for retrieval systems, which often return a single chunk to answer a query. If that chunk lacks context, the model’s answer may be vague or incorrect.

There are two common types of overlap:

  * Fixed overlap

  * Dynamic overlap


_Fixed overlap_ consistently repeats a set number of tokens or sentences between chunks. For example, if you’re using 100-token chunks and set a 20-token fixed overlap, each new chunk will begin 80 tokens after the previous one. This method is simple to implement and works well when the text structure is uniform and the transitions between chunks are predictable. It’s commonly used in systems that process structured data, such as logs or transcripts, where the size and pacing of the content are consistent.

In technical terms, if your text is being divided into chunks of 100 tokens and you move forward by 80 tokens each time, there’s a 20-token overlap between each new chunk and the one before it. This repeated portion helps preserve continuity so that important details aren’t lost when one chunk ends and another begins.

This method is great when your data is consistent and evenly structured, such as the following:

  * Log files (each entry follows the same pattern)

  * Meeting transcripts (with clear, evenly spaced dialogue)

  * Sensor data or structured reports


It’s also easy to implement with code, which makes it a popular default strategy for beginner projects or systems that handle predictable data formats. 

With _dynamic overlap_ , instead of repeating a fixed number of tokens, the approach uses content-aware rules to decide where chunks should begin and end. For example, if a chunk is about to cut a sentence in half, dynamic overlap would automatically extend the chunk boundary to include the full sentence. Similarly, if a paragraph transitions into a related idea, the chunk might stretch slightly to preserve that flow.

Dynamic overlap is particularly useful when the content relies heavily on narrative structure, logical progression, or fluid explanation, such as in technical documentation, essays, or instructional materials. These types of documents often present concepts that span several sentences or paragraphs. Dynamic overlap ensures that the model always receives enough context to understand the full idea, even if the chunk starts in the middle of a longer discussion.

For example, think about reading a story: you wouldn’t want to start reading in the middle of a sentence or stop right before the plot twist. Dynamic overlap avoids those problems by intelligently adjusting chunk boundaries.

The trade-off is that this method may produce chunks of varying sizes, which adds some complexity when indexing and retrieving them. However, the benefit is improved coherence and higher-quality retrieval results, particularly in semantically dense or conceptually layered texts.

Choosing between fixed and dynamic overlap depends on your data and goals. Fixed overlap is easier to manage and scale but may miss nuanced transitions. Dynamic overlap offers richer context but requires more sophisticated processing.

[Example 3-2](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_example_2_1755111238477473) walks through a practical example using a sliding window approach to implement overlap in Python.

##### Example 3-2. Sliding window overlap in Python
    
    
    def sliding_window_chunking(text, window_size=100, stride=80):
        """
        Splits text into overlapping chunks using a sliding window approach.
        window_size: number of tokens per chunk
        stride: number of tokens to move the window each step (controls overlap)
        """
        import tiktoken  # For tokenization (OpenAI-compatible)
     
        enc = tiktoken.get_encoding("cl100k_base")
        tokens = enc.encode(text)
     
        chunks = []
        for i in range(0, len(tokens), stride):
            chunk = tokens[i:i + window_size]
            if chunk:
                chunks.append(enc.decode(chunk))
     
        return chunks

In this example, a chunk contains 100 tokens and moves forward by 80 tokens each time, so there’s a 20-token overlap. You can adjust the `stride` to control the frequency of repetition between chunks.

# Impact of Chunking on Retrieval

Now that we’ve explored different chunking strategies and how to control chunk size and overlap, it’s time to examine why chunking matters so much in the first place, especially in the context of RAG systems. The way you divide documents into chunks affects both the _retrieval accuracy_ and the _generative quality_ of responses produced by LLMs.

When you perform a retrieval query in a RAG pipeline, you typically retrieve one or more top-ranked chunks that are semantically similar to the user query. These chunks are then passed to the language model along with the prompt. If your chunks are too large, you might include unrelated information that reduces relevance. If they’re too small, you might miss vital context needed to understand or answer the query.

The impact of chunking can be summarized in three areas:

Retrieval relevance
    

Precise chunking increases the likelihood that retrieved documents will match the user’s intent by aligning closely with the granularity of the user’s question. For example, suppose a user asks about a specific step in a process. In that case, well-defined chunks that isolate that step (such as a heading or bullet list) are more likely to be retrieved than large sections that bury the information in unrelated details. Conversely, if the chunking is too coarse—such as paragraph-level for a multi-topic article—the retrieval system may return entire blocks that include extraneous or misleading content, weakening the downstream answer quality.

Response coherence
    

The language model can only generate coherent responses if it receives enough contextual information. Inadequate chunking—such as splitting a definition across two chunks or isolating follow-up steps from their initial instruction—weakens the model’s ability to synthesize and generate logical, connected outputs. For instance, if an onboarding process is split such that _Step 1_ and _Step 2_ are stored in separate chunks, the model may generate an incomplete or incorrect response if only one step is retrieved. Maintaining semantic cohesion within a chunk ensures that the language model operates with a comprehensive understanding of the content, enabling it to deliver more natural and accurate responses.

Latency and performance
    

Finer-grained chunking increases the total number of chunks, which can significantly impact the system’s latency and computational efficiency. Each chunk must be evaluated and compared against a user query during retrieval, and a larger number of chunks increases the search space. This results in slower response times and higher processing overhead, especially when working with large datasets. For instance, in a customer support scenario where hundreds of documents are split into small 50-token chunks, the retrieval engine must scan through thousands of pieces instead of hundreds, potentially increasing query latency and compute costs. Therefore, teams must weigh the benefits of high retrieval granularity against the constraints of infrastructure and performance.

These concepts may be seen more clearly through an illustrative example.

## Real-World Example: Customer Support Knowledge Base

Imagine a company with a large knowledge base containing articles on troubleshooting software. If they chunk each article at the paragraph level (coarse chunking), a query like _How do I reset my password if I forgot my security question?_ may retrieve a long chunk that discusses account issues in general but buries the specific reset procedure midway through.

Now, consider a more refined chunking strategy that separates content based on subheadings or user-intent-aligned sections. One chunk might specifically cover “Password Recovery Options.” In this case, the retrieval system is far more likely to surface the right chunk, and the language model can generate an accurate, targeted response.

## Visualizing the Trade-off

How chunking choices affect performance for multiple parameters. [Table 3-2](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_table_2_1755111238471984) illustrates the trade-off associated with selecting specific chunking options.

Table 3-2. How chunking choice affects performance.Chunk Size| Retrieval Relevance| Context Coverage| System Latency  
---|---|---|---  
Small| High| Low| High  
Medium| Balanced| Balanced| Moderate  
Large| Low| High| Low  
  
As the table shows, chunking is a balancing act. You want to find a chunk size and an overlap strategy that maintains context while avoiding noise or inefficiency. 

###### Tip

Start with medium-sized chunks (150–300 tokens) and test retrieval performance using evaluation metrics such as precision, recall, or MRR (mean reciprocal rank). Adjust based on the specificity of your documents and user queries. 

The next section examines how to clean and filter content before chunking, including handling PDFs, noisy input, and extracting data from unstructured formats.

# Content Filtering and Extraction

Before you can begin chunking or building retrieval pipelines for a RAG system, it’s essential to clean and prepare your raw content. The quality of your input data directly influences what the language model retrieves and generates. If your documents contain noise, redundancy, or irrelevant information, even the best chunking strategy won’t prevent poor outcomes downstream.

This section covers the critical preprocessing techniques that ensure your source data is reliable, consistent, and semantically valuable. You’ll learn how to identify and remove common types of noise—like duplicated text, irrelevant headers, or formatting artifacts—and how to handle unstructured formats such as scanned PDFs and image-based documents. You’ll also explore how to convert semi-structured data into optimized, query-ready formats, such as Delta Lake tables, which enhance retrieval performance and scalability.

Content filtering is not just about removing what’s unnecessary—it’s about enhancing what matters. Whether your source is a messy document scan or a verbose product manual, applying effective preprocessing ensures that your RAG pipeline operates on high-signal content, leading to better semantic search and more accurate generative responses.

The following subsections break down this preparation process into three parts: eliminating noise, extracting content from complex formats, and preparing your cleaned data for scalable querying.

## Removing Redundancy and Noise

Before diving into document chunking or semantic indexing, it’s crucial to clean your data by removing redundant and noisy content. _Redundancy_ refers to unnecessary repetition of information, such as duplicated paragraphs, recurring boilerplate text, or templated headers. _Noise_ includes artifacts that don’t contribute to the semantic value of your document, like page numbers, HTML tags, timestamps, watermarks, or navigation menus.

In a RAG pipeline, such noisy elements can negatively affect both the retrieval and generation stages. For instance, if a chunk includes a repeated disclaimer at the bottom of every page, the retrieval engine may rank that chunk higher than more contextually relevant content, simply due to term frequency. Similarly, if boilerplate headers are not filtered out, the model may generate irrelevant or misleading completions.

Common sources of redundancy and noise include the following:

  * Web crawled documents with footers, cookie notices, and ads

  * PDFs that include repeated headers and footnotes on each page

  * Corporate policy documents that embed disclaimers and legal text in every section

  * OCR-scanned documents with garbled characters or markup tags


## Real-World Example: Software Documentation Portal

Imagine a software company indexing its entire help center for a RAG chatbot. If each article contains repeated elements, such as a navigation menu, search bar instructions, or “last updated” timestamps, these noisy parts will appear in nearly every chunk. When a user asks, _How_ _do I connect to the_ _API?_ , the retrieval engine might return chunks containing these common phrases rather than the actual API instructions, degrading the model’s relevance.

To avoid this, you must implement a preprocessing step that scans for and removes repetitive elements using pattern matching, content heuristics, or visual structure detection.

[Example 3-3](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_example_3_1755111238477492) uses regex to remove noisy patterns.

##### Example 3-3. Removing common noisy patterns using regex
    
    
    import re
    def clean_text(text):
        # Remove HTML tags
        text = re.sub(r'<[^>]+>', '', text)
        # Remove repeated disclaimers or footers
        text = re.sub(r'This document is confidential.*?\n', '', text, flags=re.IGNORECASE)
        # Remove timestamps and page numbers
        text = re.sub(r'Page \d+ of \d+', '', text)
        text = re.sub(r'Last updated: .*?\n', '', text)
        return text

###### Tip

Always inspect your documents manually before defining noise-removal rules. Noisy content often follows implicit patterns that differ across formats and organizations. 

## Extracting Content from PDFs and Images

You can extract meaningful text from PDFs and images using optical character recognition (OCR) and document parsing libraries. Many enterprise documents are stored as PDFs or scanned image files. These formats often contain valuable information, but their structure can make automated data extraction difficult. Before such content can be chunked or indexed in a RAG pipeline, it must first be converted into clean, readable text.

PDFs are challenging because they often include multiple columns, embedded images, irregular layouts, and artifacts like footnotes or page numbers. Scanned image-based PDFs pose an additional hurdle, as they require OCR to convert visual content into machine-readable text.

Depending on the document type, you might use a mix of tools:

  * PyMuPDF or pdfminer.six for extracting text from digital PDFs with standard formatting.

  * Tesseract OCR (via Python libraries like pytesseract) for scanned or image-based documents.

  * Adobe PDF Services API for more structured output, particularly from complex or formatted PDFs.


[Example 3-4](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_example_4_1755111238477511) uses the PyPDF2 library to extract the Raw data from a PDF file.

##### Example 3-4. Extracting raw text from a PDF using PyPDF2
    
    
    from PyPDF2 import PdfReader
    reader = PdfReader("sample.pdf")
    all_text = ""
    for page in reader.pages:
        all_text += page.extract_text()

If working with image-based files, OCR is essential. Example 3-5 uses Tesseract OCR:

##### Example 3-5. Extracting text from scanned images using Tesseract OCR
    
    
    import pytesseract
    from PIL import Image
    image = Image.open("scanned_doc.png")
    text = pytesseract.image_to_string(image) 

###### Warning
    
    
    OCR results can be noisy or inaccurate, especially if the scan quality is low. Always verify and clean extracted content before indexing. 

An insurance company might digitize claim documents submitted as scanned forms and PDFs. To build a semantic search system that helps agents retrieve prior claims by condition or location, the documents must first be parsed using OCR. The extracted content is then cleaned and chunked, allowing accurate retrieval based on medical conditions, policy IDs, or reported damages.

Extracting content from PDFs and images is a prerequisite step in many real-world RAG pipelines. Once complete, the cleaned data can be transformed into Delta tables, enabling structured querying, explored next.

## Converting to Delta Format for Querying

Once your source documents have been cleaned and transformed into readable text, the next step is to structure that data in a format optimized for large-scale, performant querying. Delta Lake is the preferred storage layer for this purpose in most Databricks-based RAG systems. It brings ACID transaction guarantees, scalable metadata handling, and native support for versioning—making it ideal for indexing chunked documents.

Unstructured or semi-structured data becomes much easier to retrieve and manage when stored in Delta format. After chunking, each document segment can be saved as a row in a Delta table, complete with metadata such as source document ID, chunk index, embedding vector (if applicable), and timestamp. This format facilitates efficient retrieval via SQL queries or vector similarity searches.

### Why use Delta Lake in RAG pipelines?

There are several compelling reasons to use Delta Lake for RAG pipelines:

Query efficiency
    

Delta Lake supports highly optimized reads through several advanced mechanisms. _Predicate pushdown_ enables the system to skip scanning unnecessary data by applying filters early during query execution. _Indexing_ ensures faster row access when querying specific metadata, such as chunk identifiers or document IDs. _Caching_ enables frequently accessed chunks to remain in memory, significantly reducing latency in real-time search applications. These features collectively enable scalable, low-latency querying across large datasets—a crucial requirement for responsive RAG pipelines.

Incremental updates
    

Delta Lake enables you to update specific document chunks or add new ones without requiring a complete rewrite of the entire dataset. This is especially important in RAG pipelines where data sources are continuously evolving. For instance, if a new policy document is added or a section of an existing document is revised, only the affected chunks need to be updated. This makes the pipeline more efficient and scalable, reducing processing time and storage overhead. Additionally, Delta Lake supports merge operations (upserts), which help ensure that updates are applied accurately and efficiently.

Auditability and version control
    

Delta Lake maintains a transaction log that records every change made to a table, including data inserts, updates, and deletes. This provides a complete audit trail, which is particularly important in regulated industries such as finance, healthcare, or legal services. Additionally, Delta supports _time travel_ —a feature that allows you to query data as it existed at a previous point in time. This is helpful during debugging, performance benchmarking, and model evaluation, as it enables you to compare results across different dataset versions or restore prior states after incorrect updates.

Compatibility
    

Delta format integrates seamlessly with various components of the Databricks ecosystem, making it highly versatile for machine learning workflows. For example, it connects with MLflow to track experiments and register models alongside the data used to train them, ensuring reproducibility. It works with the Databricks Feature Store to share and reuse features across projects. It also supports popular vector search libraries, such as FAISS and Databricks-native vector search, enabling high-performance semantic retrieval. This tight integration across the stack accelerates development and streamlines operationalization of RAG systems.

### Example conversion workflow

[Example 3-6](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_example_6_1755111238477544) is a simplified pipeline to convert extracted text chunks into a Delta table.

##### Example 3-6. Writing chunked documents to Delta Lake
    
    
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import monotonically_increasing_id
    spark = SparkSession.builder.getOrCreate()
    # Sample list of text chunks
    chunks = ["Chunk 1: How to configure the system...", "Chunk 2: Troubleshooting common errors..."]
    # Convert to Spark DataFrame
    df = spark.createDataFrame([(i, chunk) for i, chunk in enumerate(chunks)], ["chunk_id", "content"])
    # Save as Delta Table
    df.write.format("delta").mode("overwrite").save("/mnt/delta/rag_chunks")

###### Tip

Always include metadata fields like document title, chunk number, and source URI when creating your Delta table. This makes downstream debugging and traceability easier. 

### Real-world example: Legal document indexing

A law firm digitizes court case transcripts and legal memos. After preprocessing and chunking, each chunk is stored in a Delta table with additional metadata like case ID, date, jurisdiction, and legal category. This enables legal researchers to run precise SQL-based searches and semantic queries across thousands of cases without having to reprocess raw files.

Storing your cleaned and structured content in Delta format is the final step in preparing high-quality input data for RAG applications. With chunked text in Delta tables, you’re ready to build your retrieval layer and evaluate its performance, explored in the next section.

## Evaluating Retrieval Quality

Once you’ve prepared your chunked data and structured it in a format like Delta Lake, the next critical phase is evaluating how well your system retrieves relevant information in response to user queries. High-quality retrieval is essential in any RAG pipeline because the language model’s output is only as good as the context it receives.

This section focuses on the performance of the retrieval layer—how well your system surfaces the right chunks at the right time. Poor retrieval leads to irrelevant or misleading generations, which can severely undermine user trust and model utility. To ensure your RAG system consistently delivers valuable, accurate responses, you need to evaluate the retrieval pipeline using robust metrics and diagnostic methods.

We’ll begin by examining foundational retrieval metrics such as precision and recall, and how reranking strategies can further enhance relevance. Then we’ll look at how to interpret those metrics to improve system performance. Finally, we’ll explore common issues like retrieval gaps and how to diagnose and resolve them.

By the end of this section, you’ll be equipped with the skills to apply evaluation metrics such as precision and recall to assess how well your retrieval system is performing. You’ll understand how to fine-tune chunking and indexing parameters to improve the relevance of retrieved content, and you’ll be able to diagnose and resolve retrieval gaps using structured feedback and diagnostic techniques.

Let’s begin with an overview of core retrieval metrics and their implications in the RAG pipeline.

### Precision and recall

Understanding how to assess the effectiveness of your retrieval system is essential for building reliable RAG pipelines. Precision and recall are foundational metrics borrowed from information retrieval that help quantify how relevant and comprehensive your retrieved results are. These metrics offer a data-driven way to evaluate whether your retrieval system is surfacing the right chunks to feed into the language model.

Precision 
    

Precision measures the proportion of retrieved chunks that are actually relevant to the query. In simpler terms, it helps you understand how many of the documents or passages your system retrieved are truly useful for answering the question. For example, if your RAG system retrieves 10 chunks in response to a user query and 7 of them are directly related to the question, the precision is 70%. High precision ensures that the language model operates with accurate and trustworthy context, which is especially important in high-stakes domains such as finance or healthcare.
    
    
    Precision = (Number of Relevant Chunks Retrieved) / (Total Chunks Retrieved)

Recall 
    

Recall measures the proportion of all relevant chunks that were successfully retrieved. It answers the question: Out of everything that should have been retrieved, how much did we actually get? For example, if a query has 20 truly relevant chunks in your document store and your system retrieves 12 of them, then the recall is 60%. High recall is especially critical in scenarios where missing information can lead to incomplete or biased responses, such as academic research or regulatory compliance. A system optimized for high recall ensures that as much relevant content as possible is surfaced, even if some irrelevant chunks are included.
    
    
    Recall = (Number of Relevant Chunks Retrieved) / (Total Relevant Chunks Available)

High precision means the retrieved content is accurate, whereas high recall ensures that you’re not missing important context. In practice, there’s often a trade-off—retrieving more documents may increase recall but reduce precision.

Example scenario: Imagine you’re building a legal RAG assistant that retrieves past court cases related to a specific legal issue. If the system retrieves 10 documents, and 7 of them are relevant to the query while 3 are not, your precision is 70%. If 10 other pertinent documents of the corpus weren’t retrieved, your recall would be 41% (7 retrieved out of 17 total relevant).

### Reranking: Improving relevance

Initial retrieval is often based on vector similarity using cosine distance or dot product, but this isn’t always enough. _Reranking_ is a post-processing step where the top-N results from a vector search are re-evaluated using a more sophisticated model—often a cross-encoder or a fine-tuned BERT variant—that takes into account both the query and the full document content.

This approach improves ranking quality by modeling semantic alignment more accurately. Reranking is particularly useful when handling long-form queries or complex topics that necessitate a thorough understanding. [Example 3-7](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_example_7_1755111238477561) uses a cross-encoder to rerank retrieved chunks.

_[Example 3-7](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_example_7_1755111238477561). Using a cross-encoder for reranking retrieved chunks_
    
    
    from transformers import pipeline
    reranker = pipeline("text-classification", model="cross-encoder/ms-marco-MiniLM-L-6-v2")
    query = "What are the legal implications of data privacy violations?"
    candidate_chunks = ["Chunk A: The court ruled in favor...", "Chunk B: Data privacy laws require...", ...]
    scored_chunks = [(chunk, reranker(f"{query} [SEP] {chunk}")[0]['score']) for chunk in candidate_chunks]
    sorted_chunks = sorted(scored_chunks, key=lambda x: x[1], reverse=True)

###### Tip

Reranking improves precision significantly but can introduce latency. Use it selectively on a small candidate set (for example, the top 100 chunks). 

### When to use precision, recall, and reranking

Precision is best applied in scenarios where accuracy is paramount and retrieving incorrect information could have serious consequences, such as in healthcare diagnosis tools or legal document retrieval systems. In these contexts, surfacing only highly relevant content helps prevent the spread of misinformation or misinterpretation.

Recall is more critical in situations where missing out on relevant information is riskier than including some irrelevant data. For example, in scientific research or legal discovery, it’s important to retrieve as many potentially useful documents as possible to ensure thorough coverage of the topic.

Reranking should be used when your initial retrieval method brings back loosely relevant results that need refinement. This is especially useful in cases involving complex, multi-part queries or when user intent is subtle—like searching customer support records for nuanced user sentiment or looking up context-specific enterprise knowledge.

Together, these methods form the foundation for evaluating and enhancing the retrieval layer in your RAG pipeline.

# Using Metrics to Improve Performance

Evaluating retrieval quality is not just about reporting numbers—it’s about identifying opportunities to tune and optimize your system. This section explores how to use precision, recall, and other metrics as actionable levers for performance improvement in RAG pipelines. We’ll also examine how to identify bottlenecks, validate chunking strategies, and introduce feedback loops that continuously refine retrieval quality.

## From Metrics to Action

Precision and recall are useful starting points, but interpreting these metrics in isolation isn’t enough. The goal is to link them to specific areas of your pipeline that can be improved. For example, low precision may indicate noisy chunks or overly broad chunking strategies, while low recall could suggest gaps in document coverage or poor vector indexing.

Let’s say your RAG system shows 60% precision and 30% recall. Rather than labeling these values as simply _good_ or _bad_ , it’s important to treat them as indicators pointing to deeper systemic issues. For example:

  * If the chunks are too short, they might not contain enough context for the model to make accurate predictions. If they’re too long, key information could be diluted or buried, causing retrieval mismatches.

  * Documents might still contain irrelevant content like boilerplate legal text, navigation menus, or repetitive footers. This adds noise and reduces precision.

  * Query embeddings may not accurately capture user intent if the embedding model isn’t trained on domain-specific language. Alternatively, if the corpus was indexed using a different embedding model or encoding logic, semantic mismatches can occur.


These upstream factors create bottlenecks in retrieval quality, and identifying them allows you to test corrective strategies such as tuning chunk size, introducing preprocessing filters, or aligning your embedding models more closely with the domain vocabulary.

You can adjust chunk size, tune the similarity threshold, or experiment with different embedding models to boost both precision and recall over time. [Table 3-3](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_table_3_1755111238472005) shows the impact of chunk size on precision and recall.

Table 3-3. Impact of chunk size on precision and recall **Chunk Size** |  **Precision** |  **Recall** |  **Comments**  
---|---|---|---  
50 tokens | 0.85| 0.45| High precision, low recall  
150 tokens| 0.75 | 0.65 | Balanced trade-off  
300 tokens | 0.55| 0.80| High recall, lower precision  
  
###### Warning

Avoid over-optimizing a single metric—raising precision too much might drop recall and vice versa. Always strike a balance based on your RAG use case. 

## Validating Chunking and Filtering Strategies

Use metric-based feedback to assess the effectiveness of your chunking and content filtering strategies. For instance, test different chunk sizes and overlaps and compare their impact on retrieval quality using a validation set.

[Example 3-7](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_example_7_1755111238477561) evaluates retrieval performance using precision and recall. Suppose you have a list of relevant document IDs for a query and a separate list of document IDs retrieved by your RAG system. You want to see how many of the retrieved results are correct.

##### Example 3-7. Computing retrieval precision and recall from a validation set
    
    
    from sklearn.metrics import precision_score, recall_score
     
    # List of relevant document IDs (e.g., gold standard)
    relevant_docs = {"doc1", "doc3", "doc5", "doc6"}
     
    # Retrieved document IDs for a sample query
    retrieved_docs = ["doc1", "doc2", "doc4", "doc5"]
     
    # Create binary relevance vectors for evaluation
    true_labels = [1 if doc in relevant_docs else 0 for doc in retrieved_docs]
    predicted_labels = [1] * len(retrieved_docs)  # All retrieved docs are considered predicted relevant
     
    # Calculate precision and recall
    precision = precision_score(true_labels, predicted_labels)
    recall = len(set(retrieved_docs) & relevant_docs) / len(relevant_docs)
     
    print("Precision:", round(precision, 2))
    print("Recall:", round(recall, 2))

In this example, precision measures the proportion of retrieved documents that are relevant, and recall measures the proportion of appropriate documents that your system was able to recover.

###### Tip

Use hold-out datasets with known relevance labels to run retrieval evaluation offline. This avoids relying solely on user interaction metrics. 

## Monitoring and Feedback Loops

To sustain high performance in production, establish a monitoring loop. Track retrieval metrics over time, ideally broken down by query category, document type, or user segment. You can also introduce human-in-the-loop feedback where users label retrieved chunks as useful or not, helping refine both chunking logic and embedding models.

For example, a customer support bot could allow agents to mark helpful context chunks. These markings can then inform the retraining of the chunking strategy or reranking model, gradually improving both relevance and completeness.

Together, metrics aren’t just passive measurements—they become diagnostic and optimization tools that drive better outcomes in the RAG system.

# Troubleshooting Retrieval Gaps

Even the best-designed RAG systems will encounter retrieval gaps—moments when relevant documents are not returned, or the retrieved chunks lack sufficient context for generating meaningful responses. These gaps can significantly degrade model output, reduce trust in the system, and render downstream applications, such as summarization or Q&A, unreliable.

This section equips you with strategies to identify and resolve such retrieval blind spots. You’ll learn how to debug missing or poor results, use logs and metrics to understand failure patterns, and apply techniques to improve recall and context coverage. This is especially important in regulated or high-stakes industries, such as healthcare or finance, where incomplete retrievals can lead to inaccurate or even harmful results.

Troubleshooting retrieval gaps is not just about fixing individual cases; it’s about improving the systemic reliability of your pipeline. This section focuses on how to proactively detect weak points and implement solutions such as refining chunking strategies, enhancing indexing quality, introducing fallback query mechanisms, and combining retrieval methods to boost robustness.

Let’s begin by examining the primary causes of retrieval failure and outlining practical methods for diagnosing them in a RAG pipeline.

## Common Causes of Retrieval Gaps

Retrieval gaps typically arise when the system is unable to return relevant or complete results for a given user query. These gaps often manifest as missing, partial, or semantically unrelated chunks, and can significantly impact the performance of your RAG pipeline. Understanding the root causes of such failures is crucial to ensure reliable document retrieval, particularly in high-accuracy applications such as legal research, medical support tools, or compliance systems. The primary causes of retrieval gaps can be broadly categorized into three areas.

### Ineffective chunking

If your chunking strategy splits logical units of information across chunks—or uses sizes that are too small or too large—it can cause context loss or dilution. Granularity refers to the level of detail or size of each individual chunk in your document corpus. If the granularity is too fine (for example., small phrases or short sentences), the resulting chunks may not contain enough contextual information to be relevant on their own. On the other hand, overly coarse chunks (large sections or multiple paragraphs) may include too much unrelated content, increasing noise and reducing retrieval precision. For instance, slicing a technical specification mid-sentence can result in fragments that hold little semantic value, making them less likely to be retrieved when needed. Conversely, a large chunk covering multiple features may dilute relevance when only one specific term is queried. Choosing the right chunk size depends on the nature of your documents and the specificity of the user queries.

### Embedding mismatch

When the query embedding and document embedding come from incompatible models or domains, even semantically similar content may not align. A general-purpose embedding model is typically trained on a broad range of internet text, such as Wikipedia or Common Crawl, and aims to perform well across a wide variety of topics. However, it may not capture specialized vocabulary, stylistic nuances, or domain-specific semantics required for fields like law, finance, or medicine. A domain-specific query—for example, _interpretation of GAAP principles in earnings restatement cases_ —contains technical terminology and contextual expectations that general-purpose models may fail to encode properly. As a result, when such a query is used against financial filings or regulatory documents, the cosine similarity between the query and relevant chunks may be low, leading to poor retrieval performance.

### Indexing errors or gaps

If your document preprocessing or indexing pipeline skips important data—such as figures, tables, or appendices—your retrieval layer will lack access to this information, leading to incomplete or misleading results. This issue frequently occurs when dealing with complex documents like PDFs, especially those containing multi-column layouts, embedded images, or footnotes. In such cases, basic text extractors may misinterpret the document structure, resulting in missing or jumbled text. For example, a financial report that includes tabular breakdowns of quarterly earnings may lose critical context if the parser fails to capture the tables correctly. This can severely impact RAG accuracy when users issue queries expecting numerical or tabular data.

###### Tip

To detect whether embedding mismatch is the issue, try running manual similarity checks between your query and a few expected relevant chunks. If the similarity score is low, it’s a sign you may need to align your embedding models or fine-tune them for your specific corpus.

## Corrective Techniques to Resolve Retrieval Gaps

Once a retrieval gap is identified—whether due to chunking, embeddings, or indexing—several targeted techniques can be employed to improve overall retrieval effectiveness. The goal is not just to patch isolated failures but to enhance the robustness and adaptability of your RAG pipeline. By systematically applying corrective measures, you ensure that the system retrieves high-quality, contextually relevant information across a variety of scenarios and query types:

### Revisiting chunking strategy

If the root cause is poor chunking, consider switching to a strategy that preserves semantic boundaries. This means designing chunking logic that respects natural divisions in content—such as paragraphs, sections, or topics—rather than cutting content at arbitrary token or character counts. For example, switching from fixed-length chunking to paragraph-based or semantic chunking allows the model to retrieve more coherent and meaningful content during generation. Semantic chunking, in particular, relies on NLP techniques or model-generated embeddings to group sentences with high contextual affinity. Additionally, introducing controlled chunk overlap (where a portion of one chunk is repeated in the next) ensures that important transitional phrases or boundary content aren’t lost between adjacent segments. This is especially valuable for documents where critical meaning spans across paragraphs, such as legal arguments or multi-step procedures in technical manuals.

### Aligning embedding models

Ensure that both the query and document encoders come from the same model family or are fine-tuned on similar data. If the query encoder is trained on general-purpose internet text but the documents are specialized—say, financial statements or medical transcripts—the embedding vectors may lie in different semantic spaces, reducing their alignment. For domain-specific use cases, consider training sentence transformers using contrastive learning on labeled pairs, such as matching medical questions with diagnostic explanations or legal inquiries with corresponding statutes. This approach helps create embeddings that better reflect both the structure and vocabulary of your documents, leading to improved retrieval accuracy.

### Enhancing the indexing pipeline

Review your document parsing and ingestion process thoroughly to ensure that no relevant information is lost during the preprocessing stage. Basic text extractors may fail to capture complex layouts, embedded tables, or figures, particularly when working with scanned PDFs or image-based documents. Instead, leverage specialized tools like Azure Form Recognizer, Adobe PDF Extract API, or Amazon Textract, which can parse structured content and extract text along with layout, tables, and key-value pairs. For example, in financial reporting or clinical trial documents, tables often contain critical data that cannot be accurately interpreted without proper formatting. To preserve this context, include metadata fields (e.g., document title, author, publication date) and structure content into labeled sections or fields wherever applicable. This not only aids retrieval but also enhances downstream semantic interpretation by the RAG model.

### Query expansion and rewriting

When a user’s query retrieves few or no relevant results, several techniques can help improve recall. One approach is to apply synonym expansion, where alternative expressions or terminology are automatically added to the query. For example, expanding _heart attack_ to include _myocardial infarction_ can help match more technical content in medical records. Spelling normalization—especially important in OCR-scanned documents or user-entered inputs—can correct typos or variations in spelling (_color_ vs. _colour_). Another powerful strategy is to rephrase the query using an LLM to generate semantically equivalent variants. This technique enables the system to reformulate user intent in a manner that better aligns with document phrasing. If dense vector search still returns low scores, fallback mechanisms like keyword-based search using BM25 or TF-IDF can be activated. These sparse methods rely on exact term matching, which often succeeds when semantic search fails due to embedding misalignment or narrow vocabulary. Integrating these methods ensures broader query coverage and helps recover documents that would otherwise be missed.

### Hybrid search techniques

Combine dense vector search with sparse retrieval methods to improve both recall and coverage in your RAG system. Start by using a sparse retriever like BM25 to fetch a wide range of potentially relevant documents based on exact keyword matches. This initial set serves as a candidate pool. Then, use dense retrieval techniques—such as computing cosine similarity between embeddings—to rerank these candidates based on their semantic relevance. Optionally, you can apply a cross-encoder model that takes the query and each candidate chunk as input to score relevance more precisely. This hybrid approach ensures that even if dense models miss a document due to embedding limitations, sparse retrieval provides a safety net to catch relevant results based on lexical matches.

###### Note

While hybrid search can significantly improve coverage, it may also increase latency. Always benchmark retrieval time against your application’s performance constraints.

### Use case: Legal document search engine

A legal tech firm implemented an RAG-based assistant to help lawyers retrieve case precedents based on natural language queries. Initially, the system returned limited and often irrelevant results when users searched for nuanced legal arguments. Investigation revealed two issues:

  * Legal briefs were chunked using a fixed-length strategy that often split logical clauses across chunk boundaries.

  * The embedding model used for indexing was not trained on legal language, resulting in semantic mismatches.


To fix this, the team reprocessed the corpus using paragraph-based chunking with overlap and replaced the embedding model with a domain-adapted version fine-tuned on legal Q&A pairs. They also layered a BM25-based fallback retrieval to catch keyword matches missed by the dense retriever. Following implementation, query coverage increased by 40%, and lawyers reported a significant improvement in the relevance of the retrieved documents.

This example underscores the importance of aligning preprocessing and retrieval techniques with domain-specific content and use patterns.

### Key takeaways

We have explored retrieval gaps and how to resolve them. Here are the key takeaways from our learning:

  * Retrieval gaps can stem from ineffective chunking, mismatches in embedding, or indexing omissions.

  * You can diagnose these issues by inspecting query-embedding similarity scores, analyzing retrieval logs, and testing query reformulations.

  * Applying overlapping chunking, aligning embeddings, enhancing indexing, and hybrid search are effective remediation strategies.

  * Domain-specific use cases require tailored adjustments to maximize retrieval precision and recall.


[Table 3-3](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch03.html#ch03_table_3_1755111238472005) compares some retrieval gap strategies.

Table 3-4. 3 Diagnostic and Resolution Flow for Retrieval GapsStep| Diagnostic Question| Action to Take  
---|---|---  
1| Are chunks too small or fragmented?| Switch to paragraph/semantic chunking  
2| Are similarity scores unexpectedly low?| Align or fine-tune embedding models  
3| Are certain document sections never retrieved?| Check the indexing pipeline for omissions  
4| Do fallback queries retrieve relevant docs?| Add hybrid search to the production workflow  
  
Now that you’ve explored methods for diagnosing and correcting retrieval gaps, the next step is to solidify your understanding with a practical assessment. 

# Practice Multiple-Choice Questions

Following are multiple-choice questions to reinforce your understanding of the concepts in this chapter. These are structured to reflect the style and expectations of the Databricks Generative AI Associate certification exam. The answer key appears at the end of the questions.

  1. 1\. What is the primary reason to apply overlapping in chunking strategies for Retrieval-Augmented Generation (RAG)?

     1. A. To reduce storage costs

     2. B. To ensure chunks contain complete and continuous context

     3. C. To prevent duplication of documents

     4. D. To reduce the number of chunks required for indexing

  2. 2\. Which of the following is a drawback of using fixed-length chunking?

     1. A. It is computationally expensive

     2. B. It preserves semantic coherence across all domains

     3. C. It can break logical units of meaning, reducing retrieval accuracy

     4. D. It requires a specialized model to implement

  3. 3\. What is the main goal of filtering redundant or noisy content before indexing?

     1. A. To improve document visualization

     2. B. To reduce the number of vector embeddings

     3. C. To enhance the relevance and precision of retrieval results

     4. D. To increase the storage space utilization

  4. 4\. In which scenario would semantic chunking be preferred over sentence-based chunking?

     1. A. When processing well-structured CSV files

     2. B. When dealing with loosely organized narratives or FAQs

     3. C. When minimizing token usage is the main objective

     4. D. When using a rule-based keyword retriever

  5. 5\. Which technique would most likely resolve embedding mismatch issues in domain-specific RAG pipelines?

     1. A. Switching to fixed-length chunking

     2. B. Lowering chunk size granularity

     3. C. Using domain-adapted or fine-tuned embedding models

     4. D. Removing stop words from queries

  6. 6\. Which of the following best describes hybrid retrieval in RAG systems?

     1. A. Indexing content using only BM25-based methods

     2. B. Using both sparse and dense retrieval techniques together

     3. C. Removing fallback mechanisms to speed up search

     4. D. Separating queries by topic before performing retrieval


Table 3-5. Answer Key:Question| Answer| Explanation  
---|---|---  
1| B| Overlapping ensures that important context isn’t lost across chunk boundaries.  
2| C| Fixed-length chunking may split logical units, affecting semantic coherence.  
3| C| Removing redundant or noisy content helps improve the quality of retrieved results.  
4| B| Semantic chunking is ideal for free-form narratives where meaning spans multiple sentences.  
5| C| Domain-specific embeddings align better with specialized queries and content.  
6| B| Hybrid retrieval combines the strengths of sparse (keyword) and dense (semantic) methods.  
  
# Hands-On Lab: Chunking and Indexing for Retrieval-Augmented Generation (RAG)

## Scenario

You are a data engineer working for a health research organization. Your team is building a Retrieval-Augmented Generation (RAG) system to help researchers, doctors, and analysts query critical documents like CDC guidelines, WHO protocols, and medical articles using large language models (LLMs).

To ensure the system returns highly relevant and accurate results, you must:

  * Clean and preprocess semi-structured documents,

  * Apply appropriate chunking strategies,

  * Generate embeddings,

  * Store the results in Delta Lake,

  * And finally, index them for semantic search using Databricks Vector Search.


Your goal in this lab is to simulate a real-world pipeline and observe how chunk size and granularity impact the quality of retrieved answers.

## Objective

By the end of this lab, you will be able to:

  * Extract and clean text from public PDF medical documents

  * Apply sentence-based chunking with overlap to create coherent chunks

  * Convert chunked text into Delta format and generate vector embeddings using a hosted embedding endpoint

  * Index the embedded chunks using Databricks Vector Search

  * Perform similarity-based queries using natural language prompts

  * Simulate how chunk size affects retrieval precision using a post-query evaluation


###### Note

You can access this lab on Github at [here](https://github.com/rkaushik2007/Databricks-Certified-Generative-AI-Engineer-Associate-Study-Guide/tree/main/Chapter%203).

table of contents

search

Settings
