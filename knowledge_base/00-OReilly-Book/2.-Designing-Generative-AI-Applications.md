# Chapter 2. Designing Generative AI Applications

Generative AI applications only succeed when designed intentionally, with a deep understanding of how different components interact within a system. Designing a successful AI application goes far beyond feeding a prompt to a model; it involves mapping the business problem to the right model capability, shaping the prompt to reflect operational needs, and coordinating a series of reasoning steps that work together reliably. Without these elements, generative models can produce inconsistent, unstructured, or even misleading outputs that organizations cannot integrate into enterprise workflows.

In this chapter, you’ll go beyond basic prompting. You’ll learn how to deconstruct a business use case into logical tasks, identify the most appropriate LLM behavior for each stage, and implement modular prompt chains that can be validated, reused, and scaled. You’ll explore how to match model capabilities to structured outputs, use reasoning chains to orchestrate complex decisions, and evaluate your design using practical techniques. These skills aren’t just theoretical—they align directly with the Databricks Certified Generative AI Associate exam, which assesses your ability to reason through system design, workflow structure, and performance trade-offs in AI-driven applications.

By mastering this design process, you will gain the ability to deliver AI solutions that are not only intelligent but also robust, explainable, and business-ready.

# Prerequisites

Before starting this chapter, make sure you’re familiar with:

  * What large language models (LLMs) are and how they work at a high level

  * The concept of model inference and common output types (e.g., text, structured data)

  * Basic Python syntax and Databricks notebooks


# Learning Objectives

By the end of this chapter, you will be able to:

  * Analyze business problems and match them with appropriate LLM task types (e.g., classification, extraction, transformation)

  * Design structured and modular prompts that elicit consistent, machine-readable outputs

  * Use `PromptTemplate` and LLMChain to build and reuse prompts programmatically

  * Assemble multi-step workflows using LangChain chains, tools, and agents in Databricks

  * Evaluate pipeline effectiveness using prompt-task alignment, tool ordering, and reasoning trace outputs


# Crafting Prompts for Structured Output

Large language models (LLMs) offer flexibility, but without precise instructions, they often generate inconsistent, verbose, or unpredictable responses. For many enterprise applications—such as legal audits, financial summaries, or customer support workflows—you need model outputs that follow a specific structure, such as JSON, SQL, or tabular formats. Designing prompts that constrain and direct the model is essential for downstream integration and automation.

Let’s start with a telecom example. Imagine you’re building a support chatbot and input the following prompt:
    
    
    Summarize this support ticket

The model may return a paragraph of unstructured text. Instead, you could specify the following: 
    
    
    Summarize the following customer support ticket and return the result in JSON format, including the fields customer_id, issue_summary, and urgency_level.

Now you are guiding the model to output machine-readable, predictable content. That precision reduces ambiguity and makes integration with downstream systems, such as ticket prioritization engines or dashboards, much easier.

## Real-World Examples of Structured Prompting

Structured prompting becomes critical when business requirements demand consistent, parseable, and actionable outputs from language models. Rather than leave the model to decide the format or structure, you define the shape of the response to match what your downstream systems or human users expect. This is especially important in regulated industries or high-stakes automation pipelines where unpredictability is a liability.

Here are some business cases where structured prompting ensures reliable performance:

Legal compliance audits
    

A legal firm wants to identify contract clauses that mention termination conditions and store them in a compliance report. Instead of vague or interpretive summaries, the goal is to extract and precisely tag each relevant clause. The prompt might be as follows:
    
    
    Extract any clause mentioning 'termination' from this contract and return a JSON object with fields clause_id, text_excerpt, and risk_flag.

This approach enables the automated flagging of risk-prone clauses, speeding up the document review process.

Finance report summarization
    

An analyst requests financial insights from quarterly earnings PDFs. Rather than read lengthy prose or manually pull metrics, the analyst wants structured data suitable for entry into an internal dashboard. Here is a sample prompt:
    
    
    Summarize this Q3 earnings report in JSON format with keys: revenue, net_income, guidance, and notable_changes.

This method standardizes the analysis of financial documents across departments.

Healthcare pre-visit intake
    

For a telehealth intake form, clinicians need to extract symptoms, duration, and medical history efficiently. The LLM supports automated triage by structuring this information:
    
    
    From this patient note, extract symptom_list, duration_days, and preexisting_conditions as JSON.

Structured data entry helps standardize diagnostic workflows and enhances interoperability with electronic health record (EHR) systems.

Retail inventory queries
    

A product assistant tool receives natural language questions, such as this:
    
    
    Show me red dresses under $100.

Rather than generate a paragraph, the LLM should extract structured query fields such as category, color, and price ceiling:
    
    
    Extract product_type, color, and max_price from this query and return as a JSON object.

This technique ensures a clean query to the product catalog API.

Insurance claim validation
    

Agents may input long, detailed descriptions of incidents. Rather than summarize, the system needs to extract relevant fields such as `claim_type`, `severity`, and `fraud_flag`:
    
    
    Extract fields from this claim: claim_type, issue_description, risk_score, fraud_flag. Return as JSON.

This method accelerates downstream fraud analysis and case routing.

These use cases benefit from a consistent structure, enabling analytics pipelines, dashboards, or case management systems to utilize the model’s output without requiring human review.

## Strategies for Structured Prompting

Crafting structured prompts is not just about getting the model to respond—it’s about guiding it to produce output that’s consistent, validated, and immediately useful in a production context. Whether you are integrating with downstream systems, building user-facing features, or automating decision-making, your prompt must impose the right structure without sacrificing clarity. This section presents two core strategies you can use to steer the model toward consistent formats reliably: using delimiters and schemas and including few-shot examples.

Using delimiters and schemas
    

_Delimiters_ are clear markers that separate instructions from input, helping the model distinguish between what to process and what to follow. _Schemas_ define the structure of the desired output. For example, wrap your input in `<input>...</input>` and explicitly specify your output format:

`{document_text}`__ returns a JSON object with keys: topic, summary, sentiment.

This technique works particularly well in applications such as survey analysis or feedback processing, where you need to extract standard fields from varied inputs.

Including few-shot examples
    

_Few-shot_ prompting involves providing 1–2 examples within the prompt to illustrate the expected behavior. These examples serve as training hints, showing the model how to behave without requiring actual fine-tuning. Here’s a typical example:

Input:__`Customer reports poor audio quality during calls.`

Expected output (JSON):
    
    
     { "category": "technical", "sentiment": "negative", "urgency": "high" }

Then continue: `Now extract the same fields from the following input: {customer_feedback}`

This technique is especially useful when strict formatting is required.

###### Tip

Use double curly braces like _{{placeholder}}_ when building prompt templates. This enables easy substitution using orchestration tools or Python string injection.

[Table 2-1](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch02.html#ch02_table_1_1755111237400766) illustrates the differences between vague prompts and schema-driven prompts in terms of reliability and usability.

Table 2-1. Comparison of vague vs. structured prompts **Use Case** |  **Vague Prompt Example** |  **Structured Prompt Example**  
---|---|---  
Customer support| 
    
    
    Summarize this ticket.

| 
    
    
    Summarize this ticket in JSON format with the following fields: customer_id, issue_summary, and urgency_level.  
  
Legal compliance| 
    
    
    What does this clause say?

| 
    
    
    Extract any clause related to 'termination' and return: clause_id, text_excerpt.  
  
Financial reporting| 
    
    
    Tell me about the Q3 results.

| 
    
    
    Summarize Q3 results with keys: revenue, profit_margin, forecast, challenges.  
  
This structured approach reduces errors, improves consistency, and makes model responses easier to parse and validate. It’s particularly important when model outputs feed into automated systems, where unstructured or inconsistent results can cause failures.

## Using Prompt Templates in Code

You can implement reusable prompt templates in code using LangChain, which provides a structured way to design and manage prompts for repeated use. These templates act as blueprints that clearly define the instruction format and include placeholders for dynamic values, such as user inputs or variables pulled from a database. By centralizing the structure of your prompts, you ensure that each instance follows the same logic and formatting rules, reducing the likelihood of errors and inconsistencies.

For example, suppose you’re building a clinical note summarization system. In that case, you might have a prompt template with a static instruction, such as _“Extract diagnosis codes_ _"_ , and a dynamic placeholder for the note text. Instead of rewriting the prompt every time, you programmatically inject new note data into the placeholder, maintaining output structure across hundreds of cases.

This templated approach makes it easier to debug errors, standardize behavior across environments, and quickly scale prompt variations. It also integrates well with orchestration tools that dynamically generate prompts in response to user queries or workflow steps. [Example 2-1](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch02.html#ch02_example_1_1755111237409072) shows a structured prompt using LangChain.

##### Example 2-1. ICD-10 structured prompt using LangChain
    
    
    from langchain.prompts import PromptTemplate
    prompt = PromptTemplate.from_template(
        "Extract all ICD-10 codes from this note:\n{note_text}\nReturn them in JSON under 'diagnosis_codes'."
    )

This format allows you to dynamically plug in `note_text` while maintaining strict control over the output format. Using prompt templates in this way ensures consistency in outputs, supports rapid testing across different inputs, and facilitates easy integration of the model into larger LLM workflows and chains. It also helps centralize prompt logic, making it easier to maintain and update across multiple use cases.

[Figure 2-1](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch02.html#ch02_figure_1_1755111237393612) illustrates how a prompt template distinguishes between static text, dynamic placeholders (e.g., `{{customer_issue}}`), and the expected output schema. The system injects each placeholder programmatically before sending the prompt to the large language model (LLM).

![A screenshot of a computer

AI-generated content may be incorrect.](images/ch02_figure_1.png)

###### Figure 2-1. Prompt template structure diagram

## Matching Model Tasks to Use Cases

Designing effective prompts begins with a clear understanding of what you’re asking the language model to do—this is the _task_ _type_. Whether you’re summarizing, classifying, extracting, or transforming data, the model’s behavior must align with the specific needs of your business case. Misalignment between task type and use case is one of the most common and costly sources of failure in generative AI deployments. For example, asking a model to _“_ _summarize_ _”_ a medical report when what you need is a list of diagnosis codes results in verbose, unstructured outputs that downstream systems cannot use.

Every business scenario requires a different interaction style with the model. Some may require open-ended content generation, while others need precise data extraction. Selecting the correct task type allows you to write focused, goal-aligned prompts, apply the right evaluation criteria (e.g., accuracy, relevance, completeness), and anticipate the exact shape and structure of the model’s output. This clarity not only improves performance but also ensures smoother integration with dashboards, APIs, or workflow automation tools.

### Common LLM task types

The most common task types in business-centric LLM applications include a diverse set of patterns that align with specific operational goals. Understanding when to use each of these enables developers and product teams to avoid vague or misleading outputs and to create workflows that are modular, robust, and automatable. Here are four foundational task types that frequently appear in enterprise systems:

Text generation
    

Ideal for open-ended content creation, text generation enables the model to produce freeform text based on the given prompt. Use cases include writing product descriptions for e-commerce platforms, generating personalized emails in marketing campaigns, summarizing long reports, or drafting policy language in a consistent tone. For instance, a retail brand could use text generation to create dozens of unique yet brand-aligned product summaries from a central catalog.

Classification
    

This task involves assigning a category, tag, or label to a given input. It’s commonly applied in customer support systems to classify incoming emails by intent (e.g., _“cancel order”_ or _“technical issue”_), in HR applications to categorize job applicants, or in content moderation workflows to flag sensitive material. Classification outputs are typically short, discrete values that rule engines or dashboards can consume.

Extraction
    

Use this task when your goal is to extract structured fields from messy or unstructured text. Extraction is common in various sectors, including healthcare (e.g., extracting ICD-10 codes or symptoms), finance (extracting transaction amounts or tax IDs from scanned invoices), and law (retrieving clause types from contracts). The output is typically a JSON object, a CSV row, or a dictionary that systems can parse programmatically.

Transformation
    

Transformation tasks are essential when you need to change the format, style, or purpose of a text. These tasks include translating a query into SQL, rephrasing legalese into plain English, converting user input into API requests, or transforming bullet points into professional emails. A sales CRM might use transformation to convert call notes into a summary suitable for executive reporting.

###### Note

Task selection guides everything from prompt design to evaluation. Start by clearly defining your desired output type—free text, list, JSON, category label—and work backward to identify the right model behavior.

We can look at task selection with the help of an example of a healthcare use case where a misaligned task type can produce undesired output.

### Use case: Misaligned task type

Consider a healthcare analytics team that needs to extract diagnosis codes from unstructured doctor notes. A developer mistakenly prompts the model as if the goal were summarization:
    
    
    Summarize the following clinical note.

The output might be a paragraph of medical narrative, which does not serve the team’s needs. This mismatch between production and need is a classic misalignment—the team requires _extraction_ , not _summarization_.

Here’s a better prompt:
    
    
    Extract all ICD-10 diagnosis codes from the following clinical note. Return the result as a JSON list under the key diagnosis_codes.

This revised prompt aligns the task (extraction) with the goal (structured code retrieval), resulting in better outcomes. Misaligned prompts not only return incorrect results but also create confusion in downstream systems, which may rely on schema consistency for validation, filtering, or automation.

### Contrasting use cases: Classification vs. extraction

To fully appreciate the distinction between classification and extraction, it’s helpful to first ground these concepts in the context of a real-world workflow. Consider an enterprise HR department or legal team that processes large volumes of text data every day—from resumes to contracts to policy documents. These teams often face multiple questions: How do we sort this content for the right reviewer? How do we extract the key details without manual effort? Such scenarios**** are where choosing between classification and extraction becomes critical. We’ll walk through two examples in detail: HR and legal operations.

Let’s examine the differences between classification and extraction when applied in enterprise contexts by comparing their goals, outputs, and the prompts that activate each task. 

_Classification_ involves assigning a predefined label to a piece of input data, often to support routing, filtering, or analytics. _Extraction_ , on the other hand, consists of extracting specific pieces of information from unstructured text to populate structured data systems. Understanding the contrast between these two task types helps ensure that LLM applications are scoped correctly and deliver predictable, actionable results. Now let’s understand it with two examples:

HR resume analytics
    

In the HR domain, companies receive a flood of resumes daily. The challenge is not only identifying the right candidates but doing so at scale, with consistency. LLMs can assist with both routing and data parsing depending on the specific prompt structure.

Classification
    

The goal here is to sort each resume into a predefined role category (e.g., software engineer, data analyst, product manager**).** This categorization is useful for automating job pipelines. Prompt:

`Classify this candidate’s resume as a software engineer, data analyst, or product manager.`

Output: A discrete label, like _software engineer_ , suitable for filtering or queueing.

Extraction
    

Instead of just labeling, you may need to extract detailed fields from each resume for an applicant tracking system, such as years of experience, degrees, or technical skills. Prompt:

Extract `years_of_experience, education_level, and skills from this resume. Return as JSON.`

Output: Structured key-value pairs that support analytics, search filtering, or candidate ranking.

Legal contract analytics
    

Legal teams frequently review and analyze lengthy contracts that contain numerous clauses. Each clause may pertain to different legal functions, such as termination, liability, or payment terms. Prompting LLMs correctly enables both contract analysis and downstream compliance automation.

Classification
    

The first step might be labeling the paragraph or clause based on its legal type. This categorization enables the rapid identification of areas where to focus legal review. Prompt:

Classify `the following paragraph as liability, payment, or termination.`

Output: A category label, e.g., _termination_ , to organize sections or prioritize legal reviews.

Extraction
    

After classification, you may need to extract specific obligations, such as a due date or monetary value. Prompt:
    
    
    Extract the payment_amount and due_date from this contract clause in JSON.

Output: `{ "payment_amount": "$5,000", "due_date": "2025-09-01" }`_—_ structured data that feeds into a compliance tracker or risk dashboard.

### Choosing the right model task

Now, you might be wondering how to choose the right model tasks. Ask the following guiding questions to select the most appropriate LLM task for your specific business scenario. This structured approach helps reduce ambiguity and ensures that your prompts trigger the right type of model behavior, increasing accuracy and integration readiness:

  1. Do you need the original text that wasn’t in the input?

  2. Use text generation. If your goal is to create content that expands on or introduces new ideas not found in the input, such as writing a product description or composing an email response, then text generation is the right choice. It enables the model to synthesize information, emulate tone, or personalize messages.

  3. Do you need to convert input from one form to another?

  4. Use transformation. If you’re translating between formats (e.g., plain English to SQL, bullet points to email), the transformation preserves the core content while adapting the format, tone, or structure. This approach is ideal for workflows that require standardization, localization, or format-shifting.

  5. Are you pulling specific fields from unstructured text?

  6. Use extraction. When working with dense, unstructured data (e.g., clinical notes, invoices, contracts) and you need to extract specific attributes into a consistent structure, such as JSON or CSV, extraction is the ideal approach. It’s particularly important for structured analytics, automation, or integration with databases.

  7. Do you want to assign a category or label to the input?

  8. Use classification. Use this when your goal is to route or filter input into one of several predefined options, such as tagging sentiment, categorizing an email, or determining the topic of a document. This task is well-suited to building logic flows, triggering business rules, or summarizing input types.


[Table 2-2](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch02.html#ch02_table_2_1755111237400796) shows some common use cases. Each prompt should align with both the technical capability of the model and the practical needs of the system consuming the output.

Table 2-2. Common use cases and recommended LLM tasks **Use Case** |  **Desired Output** |  **Recommended LLM Task** |  **Prompt Format Suggestion**  
---|---|---|---  
Customer service routing| Category label| Classification| 
    
    
    Categorize this message as: billing, technical, or general.  
  
Financial KPI extraction| Structured JSON| Extraction| 
    
    
    Extract revenue, profit, and margin from this report in JSON.  
  
Legal clause transformation| Rewritten clause| Transformation| 
    
    
    Rewrite this clause in plain English for a layperson.  
  
Product description generation| Natural language text| Text generation| 
    
    
    Write a brief product description for this camera.  
  
SQL query from natural language| SQL statement| Transformation| 
    
    
    Convert the following question into a SQL WHERE clause.  
  
Email intent detection| One-word intent (e.g., inquire)| Classification| 
    
    
    Identify the intent of this email: {email_text}  
  
### Best practices for prompt-task matching

Designing an effective generative AI system involves more than just selecting a task type; it also requires considering the specific characteristics of the task. Even with the correct classification or extraction strategy, poor prompt-task alignment can lead to unreliable outputs. Certain best practices help ensure that model behavior remains predictable, reproducible, and well-integrated into your business pipeline. Consider the following guidelines to make your prompt-task pairings robust and production-ready:

Start with the desired output format
    

Before writing your prompt, consider what the receiving system expects. Will the output feed into a user interface, an analytics dashboard, a reporting engine, or a downstream API? For instance, a chatbot may need human-readable sentences, whereas a billing system may require JSON objects. Working backward from the expected output allows you to constrain your prompt more effectively and ensures that the format is immediately usable. Will the output feed into a UI, a SQL engine, or an analytics dashboard? Align the prompt design to produce that output type.

Clarify downstream consumers
    

You should always know who or what will consume the model’s output. Is it a human support agent, a financial analyst, or a backend service? A report intended for stakeholders might benefit from using structured bullet points and domain-specific terminology. On the other hand, automated systems might require machine-readable formats with exact key-value pairs. This knowledge will help shape not only the format but also the level of detail, tone, and even vocabulary used in your prompts. Understand who or what uses the model’s output. If it’s an API integration, structured formats like JSON are often required. If it’s a human reviewer, well-formatted text or bullet points may suffice.

Differentiate between open-ended and closed-ended tasks
    

An open-ended task, such as generating a creative ad slogan, demands a different strategy than a closed-ended task, like labeling sentiment. Open-ended prompts benefit from rich context, tone cues, and few-shot examples to help steer the model toward stylistic or domain alignment. Closed-ended prompts, on the other hand, require strict constraints, clear format expectations, and sometimes even schema validation rules to avoid hallucinated or malformed output. Open-ended tasks (like text generation) thrive on few-shot examples and tone-setting context. Closed-ended tasks (like classification or extraction) require a tighter structure and often benefit from explicit formatting instructions.

Make your intent explicit
    

Models interpret language probabilistically. If you are vague in your instructions, the model may interpret it in ways you didn’t intend. For example, saying _Summarize the ticket_ is open to wide interpretation—does the user want a headline, a paragraph, a bullet list, or key fields? Instead, specify: _Return a JSON object with fields: issue_type, urgency, and resolution_steps_. The clearer your intent and constraints, the more predictable and reproducible the model’s behavior will be. The model should never have to guess your intent. Specify what to extract, how to format it, and what the output should contain. This reduces ambiguity and improves reproducibility.

Iterate and test with edge cases
    

A prompt that works in ideal conditions may break when confronted with real-world variability. Always test across a wide range of inputs, including missing values, poorly formatted data, contradictory information, and edge-case phrasing. This testing helps uncover failure modes early, ensures the model is robust across various conditions, and enables you to optimize prompt design and fallback strategies. Use prompt evaluation frameworks or Databricks notebook logging to benchmark consistency across test cases. Always run your prompts against varied inputs, including incomplete, ambiguous, or borderline examples. This strategy helps validate whether your prompt and task match will hold under real-world conditions.

Tools like LangChain and Databricks Prompt Engineering UI allow you to build reusable, task-specific prompt modules and chain them together. For example, a pipeline might begin with a classifier to identify the request type, send specific inputs to an extractor for field parsing, and then transform the output into a readable summary. Matching the task to the prompt ensures reliability, modularity, and clarity at each step of the pipeline.

### Aligning prompts to use cases

_Prompt-task alignment_ refers to the practice of matching the model’s behavior (task type) to the specific goal of a business use case. For example, if your objective is to extract structured data from unstructured text, you must use an extraction task with a prompt tailored to that need. Misaligned prompts—such as requesting a summary when a structured output is needed—can yield irrelevant or unusable results. Aligning the prompt to the task ensures the model delivers consistent, actionable, and format-compliant outputs that meet the expectations of downstream systems or users.

Let’s consider the importance of prompt-task alignment with a healthcare example. Let’s say you work with a healthcare team analyzing clinical notes. A prompt reads as follows:
    
    
    Summarize this clinical note.

The model may generate a natural language paragraph that includes observations and assessments. However, if your goal is to extract specific ICD-10 diagnosis codes, this is not helpful. Instead, the correct prompt and task would be more like this:
    
    
    Extract all ICD-10 diagnosis codes from the following note. Return them in a JSON list under the key diagnosis_codes.

By aligning the model task (extraction) with the use case (code retrieval), you get consistent outputs that are easier to validate and use downstream.

### Task type decision matrix

Each business use case typically maps to a specific model task type—such as classification, extraction, or transformation—and demands a carefully tailored prompt format. Choosing the right task and prompt structure is critical to producing outputs that are consistent, actionable, and aligned with downstream requirements. [Table 2-3](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch02.html#ch02_table_3_1755111237400816) helps to decide common enterprise use cases with the correct LLM task type as a quick reference to help determine which task type and prompt style best fit your use case.

Table 2-3. Task type decision matrixUse Case| Desired Output| Task Type| Prompt Format Example  
---|---|---|---  
Support ticket routing| Category label| Classification| 
    
    
    Classify this ticket as: billing, technical, or general.  
  
Financial KPI extraction| JSON object| Extraction| 
    
    
    Extract revenue, expenses, and margin from this report in JSON.  
  
Clause simplification (legal)| Rewritten text| Transformation| 
    
    
    Rewrite this clause in plain English.  
  
Product description generation| Natural language| Text generation| 
    
    
    Write a brief product description for this item.  
  
Convert query to SQL| SQL statement| Transformation| 
    
    
    Convert this natural language query into a SQL WHERE clause.  
  
Email sentiment detection| One-word sentiment| Classification| 
    
    
    Identify the sentiment of this message.  
  
# AI Pipeline Composition

Composing an effective AI pipeline means assembling modular, interoperable components that collaborate to transform user inputs into useful, structured outputs. These pipelines are foundational in enterprise-grade AI systems because they enforce repeatability, enable debugging, and support governance. By orchestrating tasks like prompt formatting, model invocation, and memory management into discrete steps, you gain more control over the AI’s behavior and performance. This approach is especially critical in regulated industries, high-volume use cases, and applications where auditability matters.

Designing effective generative AI workflows involves more than just crafting a clever prompt. In real-world production systems, you must build an orchestrated sequence of functional steps—known collectively as an AI pipeline. An _AI pipeline_ is a structured flow of tasks where each component is responsible for a specific function, such as preparing input, formatting prompts, calling the LLM, parsing output, and preserving conversational memory.

To illustrate, consider a typical AI pipeline for an insurance claim assistant:

  * The pipeline begins by accepting user input, such as a written description of a claim.

  * A prompt template standardizes this input into a well-formed instruction.

  * The LLM wrapper receives the instruction and forwards it to the large language model for processing.

  * An output parser captures the model’s response to ensure it is machine-readable (e.g., in JSON format).

  * A context manager tracks the conversation state, allowing follow-ups to reference previous messages.


Think of this pipeline as a team of workers on an assembly line: one prepares the raw materials, another shapes them, another inspects the result, and yet another tracks the progress. Each step enhances the reliability, traceability, and scalability of your AI workflow.

This section breaks down these components in detail, demonstrates how they operate using LangChain, and explores best practices for assembling pipelines using Databricks tools.

## Selecting Chain Components

A generative AI pipeline is composed of modular building blocks, where each component handles a distinct function in processing data and interacting with a language model. These responsibilities may include formatting prompts, executing model queries, parsing outputs, and managing session context. Much like a traditional software pipeline that validates inputs, processes logic, and formats outputs, an AI pipeline ensures that every step in the interaction with the model is controlled and reproducible. Choosing the right components—and assembling them in the right order—is essential for creating scalable, maintainable, and production-grade AI applications. This modularity enables easier debugging, targeted improvements, and safer deployment in real-world environments where consistency and transparency are crucial.

### Core pipeline components

Core pipeline components form the backbone of any AI application. These modules handle tasks such as formatting prompts, executing model queries, parsing responses, and maintaining context across interactions. When combined, they enable reliable, scalable, and modular AI systems that can be reused and maintained across use cases.

Prompt templates
    

These serve as structured blueprints for your prompts, transforming ad hoc instructions into standardized forms. Each template includes dynamic placeholders (e.g., {{user_query}}, {{product_type}}) that can be filled in programmatically. Using templates with dynamic placeholders**** makes prompt creation reusable and testable. Templates reduce errors by enforcing a known schema, help scale prompt logic across teams, and are especially effective when building applications like chatbots, fraud detection tools, or summarizers.

LLM wrappers/model runners
    

These modules interact directly with the underlying language model (e.g., OpenAI, Mistral, Cohere). They manage configurations such as:

  * _Temperature:_ It helps control randomness, which balances creative and conservative responses.

  * _max_tokens:_ For limiting output length, ensuring that the model doesn’t return excessive or irrelevant information.

  * _model_name_ : To specify which version of the LLM to use. Wrappers enable retry logic, timeouts, logging, and monitoring, all of which are critical for ensuring production reliability.


Output parsers
    

These tools take the model’s raw text output and transform it into structured formats, such as JSON, lists, or tables. This step ensures that downstream applications, such as business dashboards, analytics engines, or user interfaces, easily consume the data.

Memory or context managers
    

These components preserve context across user interactions. Memory types include the following:

  * _Buffer memory:_ Buffer memory stores the last few interactions in their original form, preserving the exact wording of recent exchanges between the user and the AI. This type of memory is ideal for short conversations or applications where precision in recalling the latest statements is critical, such as technical support bots or form-filling assistants. Since it does not summarize or alter the message, it ensures high fidelity in tracking immediate context, making it useful when follow-up queries rely heavily on the exact phrasing of previous inputs.

  * _Conversation summary memory:_ This memory summarizes previous messages to save space while preserving the original intent. This type of memory condenses previous dialogue into a concise summary that retains key points and decisions from earlier turns. It is especially useful in long conversations where tracking exact wording isn’t necessary, but understanding the overall topic and intent is. For example, a healthcare triage chatbot might summarize a patient’s ongoing symptoms and history in one or two lines, allowing the model to stay on topic without being overloaded by lengthy transcripts.

  * _Entity memory:_ This memory type tracks key named entities, such as people, dates, organizations, product names, or locations, mentioned throughout a conversation. It allows the model to retain specific facts and references across multiple turns. For example, in a customer support scenario, if a user mentions _John Smith_ and _order #12345_ early in the exchange, entity memory ensures those details are remembered and reused in follow-up responses or summaries. This capability enhances coherence, personalization, and task continuity in applications like CRM chatbots, virtual assistants, and automated interviewers.

Another example: if the user first says, _Show me laptops under $1000_ and then follows with, _How about with 16 GB RAM?_ the memory system ensures the second request builds on the earlier one without losing context.


These pipeline components are assembled in series or conditionally branched depending on the application logic. Each serves to isolate functionality, improve maintainability, and enhance testability.

### Example using pipeline components

Let’s say a customer types: _Can you recommend a few wireless headphones under $100?_

Here’s how each pipeline component might operate:

  1. The prompt template fills in a blueprint:
         
         Find products related to {{product_type}} that cost under {{budget}} dollars.
                         

Here, _wireless headphones_ and _100_ populate the placeholders.

  2. The LLM wrapper __ submits this structured prompt to the model and controls how creative or focused the response should be.

  3. The output parser takes the list of recommended products and transforms it into a JSON array with fields such as _name_ , _price_ , and _rating_.

  4. The memory manager keeps track of the last query. If the user later asks, _What about noise-cancelling ones?_ the system remembers the original category and price context.


## Summarizing Claims with LangChain

Let’s implement a simplified claims summarization workflow using LangChain to illustrate the concepts you learned in this section. You are working in an insurance organization and need to process incoming claims written in natural language. These could be user-submitted messages, such as _S_ _lipped on a wet floor and reported back pain._

Rather than passing this text directly to an LLM for general interpretation, you want a system that always returns a structured summary with keys like `claim_type` _,_`urgency`, and `issue`. This strategy ensures consistency across responses, simplifies validation, and enables seamless downstream integration (e.g., into a dashboard, claims processor, or audit tool).

To build this, you’ll need the following:

  * A prompt template that outlines the desired response structure.

  * An LLM wrapper that handles interaction with the model.

  * A chain that connects them and feeds the input to the model.


Here’s how that looks in the code:
    
    
    from langchain.prompts import PromptTemplate
    from langchain.llms import OpenAI
    from langchain.chains import LLMChain
     
    # Create a prompt template for summarizing insurance claims
    summary_prompt = PromptTemplate.from_template(
        "Summarize this insurance claim and return a JSON with keys: claim_type, urgency, and issue: {claim}"
    )
     
    llm = OpenAI(temperature=0)
    summary_chain = LLMChain(llm=llm, prompt=summary_prompt)
     
    # Run the chain with a sample input
    result = summary_chain.run("Patient slipped on wet floor and reported back pain.")
    print(result)

The pipeline takes a free-text claim, converts it into a structured summary, and enables logging, auditing, or routing to human adjusters for further review. [Figure 2-2](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch02.html#ch02_figure_2_1755111237393648) illustrates this process.

![A blue rectangles with black text

AI-generated content may be incorrect.](images/ch02_figure_2.png)

###### Figure 2-2. AI pipeline workflow

###### Tip

Avoid writing a single, comprehensive prompt that attempts to cover everything at once. Break tasks into smaller steps, each with its own component. This improves accuracy, reusability, and traceability.

## Template Registries and Reuse

Managing pipeline elements at scale becomes increasingly important as organizations deploy more GenAI apps across departments. Without reusable assets and shared governance, prompt logic and workflow definitions become fragmented and difficult to maintain. That’s where prompt registries come in—designed to support discovery, consistency, and versioning of reusable pipeline assets.

In enterprise teams, managing prompt templates and component logic at scale requires governance to ensure consistency, reusability, and auditability. As organizations build more LLM-powered applications across various functions, such as customer service, compliance, and analytics, the number of prompts and chains can grow rapidly. Without a system for organizing and maintaining these templates, teams risk duplication, inconsistencies, and difficulty tracking updates.

One effective solution is to centralize prompt templates in a shared _registry_ —a version-controlled repository that stores standardized templates, tracks changes, and enforces access permissions. This registry functions like a library, allowing teams to search for, reuse, and test approved templates without reinventing the wheel each time. By tagging templates with metadata such as `domain`, `task type`, or `version`, large teams can scale their prompt engineering practices while maintaining high quality and traceability.

## Pseudo-UI for Prompt Registry

These registries promote reuse by enabling teams to share proven templates across projects and departments, thereby facilitating efficient collaboration. Version control allows developers to track changes, roll back to earlier states, and understand how prompts evolve.Registries also streamline testing by integrating with automated pipelines to validate prompt behavior across diverse inputs. Altogether, this infrastructure ensures that pipelines remain reliable, easier to debug, and aligned with compliance or quality standards, especially as the scale and complexity of AI systems grow. [Table 2-4](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch02.html#ch02_table_4_1755111237400833) shows a sample prompt design registry.

Table 2-4. Sample prompt design registryTemplate Name | Description   
---|---  
_summarize_claim_v2_ | Insurance claims   
_detect_fraud_scenarios_ | Fraud detection   
_support_ticket_classifier_ | Customer service   
  
A well-designed AI pipeline breaks down complexity into manageable parts. By composing prompt templates, LLM wrappers, output parsers, and memory modules, you create systems that are easier to maintain, more interpretable, and better aligned with real-world workflows. LangChain and Databricks tools make these compositions easier to implement across diverse enterprise scenarios. 

###### Warning

Avoid monolithic prompt chains that try to do everything in one shot. Break down complex flows into modular parts to improve accuracy, reusability, and debuggability.

## Translating Use Cases into Input/Output Design

Translating business use cases into a well-structured AI pipeline begins by identifying and formalizing the system’s expected inputs and outputs. This step is foundational because the effectiveness of your AI solution hinges not only on what the model does but also on how it receives data and produces formatted outputs for consumption. You can think of this process as building a bridge between human understanding and machine processing: the model must understand the input in a way that it can comprehend and produce the output in a structure that downstream systems or users can act upon.

This input/output (I/O) formalization ensures that your AI workflow is logically coherent, consistently structured, and fully interoperable with any downstream systems, such as business intelligence dashboards, CRM software, rule engines, or even human-facing applications. Without this clarity, your pipeline might produce technically valid results that are unusable in context, leading to breakdowns in automation, compliance failures, or poor user experience.

The goal is to map each use case to a clear, testable _I/O contract_ —a structured agreement that defines the shape, structure, and content expectations for both the inputs the model receives and the outputs it produces. This contract enables modular pipeline components to operate predictably, reduces ambiguity in prompt design, and provides a clear basis for evaluating model performance and business impact.

## Input/Output Design Use Case

Without clearly defined inputs and outputs, even well-engineered AI pipelines can break down, resulting in unusable data, inconsistent results, or failure to integrate with existing tools. Defining your I/O design upfront makes it easier to do the following:

  * Align with the expectations of business stakeholders

  * Reduce ambiguity in prompt formulation

  * Create test cases that validate performance

  * Ensure compatibility with APIs, dashboards, and databases


Let’s walk through a concrete use case of an Insurance Claim Validator app in more detail. Imagine you’re part of a team at an insurance company tasked with reducing the manual workload of reviewing incoming claims. Today, human agents must read through scanned documents or typed descriptions, classify the type of claim, verify whether all required information is present, and determine if the claim complies with coverage rules. This process is time-consuming and prone to inconsistency.

An AI-driven system can streamline this by automatically parsing input documents, detecting key information, validating claim completeness, and offering clear next-step recommendations. The system must handle multiple formats and extract structured information from unstructured inputs, all while preserving compliance and auditability. The business objective, therefore, is to ensure high-throughput, consistent claim validation, with flagged exceptions routed to human reviewers.

To design this workflow, we will perform the tasks in the following subsections.

### _Decomposing the task into multiple steps_

Decompose the task as follows:

  1. Utilize optical character recognition (OCR) or document AI to extract text from scanned documents. This process involves using pre-trained models or services to digitize and clean text from PDFs or image files. The extracted text forms the raw data that feeds into the next steps.

  2. Classify the claim (e.g., auto, medical, property). This step uses a classification model or prompt-based logic to assign a category to the claim, which may affect how validation rules are applied later.

  3. Validate the content against rules (e.g., coverage, missing fields). Based on the claim type, a set of business rules or regulatory criteria is applied. This validation process may involve checking for required fields, coverage eligibility, and data inconsistencies.

  4. Recommend next steps (e.g., approve, escalate, request more info). The final stage involves applying logic—either LLM-based or rule-based—to suggest the next course of action. The output might include a recommendation flag, confidence score, or specific instructions for a human reviewer.


### Define the inputs

Define the inputs**** as follows:

Raw text from scanned documents
    

Typically pulled using OCR or document AI tools, this text includes unstructured content, such as medical notes, accident descriptions, or property damage details, that must be parsed and interpreted.

Metadata
    

Includes fields such as `claim ID`, `submission date`, `customer ID`, and `document timestamps`. These contextual signals help the AI system anchor its reasoning and make connections across different data fields.

Optional structured form entries
    

If available, this includes drop-down selections, checkboxes, or pre-filled values submitted through digital forms. These provide high-confidence data points that can complement or validate the information extracted from unstructured text.

### Specify the outputs

Specify the outputs as follows:

A structured JSON object—such as the one shown next—helps standardize outputs for downstream processing, validation, and visualization. In enterprise environments, this structure is crucial because it enables various components—such as APIs, analytics dashboards, or workflow engines—to consistently and reliably consume model outputs.

Each designed JSON field represents an actionable insight or data point relevant to the insurance validation process. For instance, `claim_type` aids in routing logic, `is_valid` is crucial for compliance checks, `issues_found` helps flag anomalies for human review, and `recommendation` provides operational guidance. By clearly defining these fields in advance, you ensure that the AI system not only performs its task but also communicates results in a structured and actionable way.

Here is the JSON:
    
    
    {
      "claim_type": "medical",
      "is_valid": true,
      "issues_found": ["missing policy number"],
      "recommendation": "Request policy number from customer"
    }

### Map to pipeline components

Map to pipeline components as follows**:**

Document parser for OCR
    

This component is responsible for digitizing the contents of scanned claims and extracting unstructured textual data. It utilizes tools such as Azure Form Recognizer, Google Document AI, or the open-source Tesseract to convert images or PDFs into machine-readable text.

Prompt Template and LLMChain for claim classification
    

After text extraction, this step involves crafting a prompt that guides a language model to identify the type of claim, such as _auto, medical, property_ , etc., based on specific keywords, patterns, or contextual information. It feeds into a LangChain LLMChain or a custom classification model that outputs a structured label.

Rule-based filter for validation
    

This module applies deterministic business rules or heuristics to the extracted and classified information. For example, if a medical claim is missing a required diagnosis code or if a property damage claim doesn’t specify the location, it flags the issue for review. This layer ensures consistency and enforces compliance logic.

### Summary generation with prompt template

In the final phase, the system uses a tailored prompt to summarize its findings—such as validation status, detected anomalies, or rule violations—and generate actionable guidance. For example, it might confirm that a claim is valid but missing a policy number, then recommend asking the customer for that information. This step ensures the system communicates results to either a human reviewer or another automated process for final decision-making or escalation, such as requesting additional details or recommending approval. The summary is typically presented to a human agent or passed to a downstream system for further processing. You can prototype and validate this flow using Databricks notebooks. Prompt chaining tools, such as LangChain, or orchestration platforms like MLflow Pipelines, allow you to modularize each step and fine-tune prompts, models, and validation logic incrementally.

## Tips for Robust I/O Design

Translating a business use case into an AI pipeline isn’t just about choosing a model—it’s about engineering reliability in every step. The following best practices help ensure your input and output handling is consistent, transparent, and production-ready:

Design for failure modes
    

Always anticipate edge cases—such as missing fields, invalid formats, inconsistent date values, or overlapping categories. Build your pipeline to detect and gracefully handle these scenarios by including fallback mechanisms or default behaviors. For example, if a required field is missing, flag it for review instead of letting the system fail silently. This process increases fault tolerance and enhances the system’s trustworthiness.

Log intermediate steps
    

Capturing intermediate input and output at each step of the pipeline, such as post-OCR extraction, post-classification, and post-validation, enables you to audit decisions, trace back the root cause of issues, and continuously improve your AI system. It also supports better debugging and compliance, especially in regulated industries where audit trails are essential.

Standardize output schema
    

Define and adhere to a fixed output schema with clearly named keys, using conventions such as camelCase or snake_case. Standardization ensures seamless compatibility with downstream systems, including APIs, business dashboards, and monitoring tools, thereby facilitating efficient integration. It also enables schema validation, which helps prevent runtime errors caused by unexpected field formats or names.

Visualize the flow
    

Create a visual representation of your I/O logic to enhance team communication and foster stakeholder alignment. Use tools like Mermaid.js for Markdown-based diagrams, Lucidchart for detailed flowcharts, or built-in Databricks visual components. Visualizations clarify dependencies and help identify opportunities for optimization within the pipeline.

###### Tip

Work backward from your desired output. Knowing exactly what form your output must take—such as a specific JSON schema or SQL row—makes it easier to design prompts, select tools, and structure inputs to align with those goals. By formalizing input/output design for each use case, you not only make your pipeline more testable and maintainable, but you also align it more closely with real-world business requirements. Always test your pipeline with edge cases, such as missing data or ambiguous inputs. This helps you refine prompt clarity and improve robustness before deployment.

## Designing Reusable Prompt Templates

To get consistent results from a language model, you must use prompts that follow a well-defined structure. Ad hoc instructions often yield unpredictable or subpar outputs. In contrast, reusable prompt templates enable you to standardize your communication with models across various tasks, teams, and products. This consistency not only improves performance but also streamlines collaboration.

A reusable prompt template serves as a blueprint that guides the model to respond predictably and coherently. It enables teams to reuse and modify a common format rather than writing prompts from scratch each time. These prompt templates are particularly useful in enterprise environments where compliance, brand voice, or response structure matters. On the Databricks Certified Generative AI Associate exam, you’ll need to identify the components of a prompt template and evaluate their effectiveness in real-world deployments.

These templates enhance output quality by reducing variance, ensuring consistency in tone and formatting, and making model behavior easier to interpret, troubleshoot, and refine. They support a wide range of tasks, including customer query resolution, document summarization, data transformation, and code generation. In complex workflows, reusable prompts also make it easier to integrate models into automated systems or chains of operations without the need for constant rewriting.

Effective prompt templates typically include the following elements:

Instruction
    

This is a clear and concise directive that tells the model exactly what action to perform. It sets the purpose and tone for the response. For instance, prompts like _Summarize the following review_ or _Classify the sentiment of the text as Positive, Neutral, or Negative_ provide the model with a specific task.

Context or example(s)
    

These are additional cues or demonstrations that help the model understand the expected output. In few-shot prompting, you might show two or three example inputs and outputs to guide the model’s behavior. This process boosts performance for specialized or nuanced tasks.

Input
    

This refers to the unique content or query that the user provides at runtime. It could be a customer complaint, a product description, a data snippet, or any other dynamic content. A reusable template will swap this content into a designated placeholder.

Format or output constraint
    

This specifies how the model should structure its response. It may include style instructions (_Write in a professional tone_), formatting rules (_Use bullet points_), or data types (_Respond in JSON with keys for title and summary_). Output constraints make integration with downstream systems more predictable.

For example, in a retail chatbot scenario, you might create a reusable template like this:
    
    
    Instruction: Provide a helpful and friendly answer to a customer's question.
    Example:
    Customer: "Do you offer free shipping?"
    Answer: "Yes! We offer free shipping on orders over $50."
    Customer: {{question}}
    Answer:

You can fill in the variable _{{question}}_ with dynamic input from a live system. Because the structure remains consistent, you can reuse this template to power many different queries while maintaining a uniform tone and format. 

###### Tip

Keep prompts modular. You can combine building blocks (like examples, instructions, and formats) to fit specific tasks. This combination makes your system more maintainable and flexible as use cases evolve.

Reusable prompt templates act like the API contracts of LLMs. They help you frame the model’s “interface” in a way that stays consistent, understandable, and easy to update. As you build more complex applications in Databricks, prompt templates will become essential to quality control and scalability.

# Multi-Stage Reasoning Tools

When solving complex tasks, a single prompt often overwhelms the model by asking it to perform too many operations simultaneously, such as extracting facts, interpreting them, and presenting conclusions. This results in vague, shallow, or error-prone responses. For instance, if you prompt an AI to _Read this quarterly report and write a detailed summary of its financial implications_ , you’re effectively bundling four or five tasks into one: reading, understanding, extracting key data, analyzing trends, and summarizing. The model may respond inconsistently or miss critical insights because it generalizes across multiple mental steps.

Now consider a structured, multi-stage approach with three prompts:

  * Extract financial metrics using a prompt tailored for extraction.

  * Analyze the extracted metrics to identify patterns or trends.

  * Summarize the insights using a prompt designed for clear, investor-friendly language.


Each prompt does one thing well. By narrowing the model’s scope at each stage, you increase reliability, transparency, and reusability. You can inspect and validate outputs at every step, quickly identify errors, and reuse components across different workflows. This technique, known as _multi-stage reasoning_ , mirrors how people solve problems—by progressing step-by-step with a clear focus.

This section introduces two core strategies for implementing multi-stage reasoning:

  * Tool ordering _**,**_ where you explicitly define each stage in a fixed sequence.

  * _Agent planning_ _**,**_ where dynamic decisions determine the flow based on context


Both strategies appear in the Databricks Certified Generative AI Associate exam and are foundational to building scalable, modular LLM systems.

## Tool Ordering for Sequential Logic

_Tool ordering_ is the process of arranging tools or prompts in a logical sequence. When you complete tasks in the right order, your workflow produces clear and dependable results. Think of it like a recipe: if you try to frost a cake before baking it, the outcome will fail.

In AI, tool ordering is critical. For example, consider building a financial assistant that answers questions about a company’s performance:

  1. Search recent earnings PDFs using a document retriever. This stage ensures that you gather the most relevant and current documents based on user input or keywords. The retriever acts as a search engine, pulling in documents from a local repository or cloud source to feed into the next stage.

  2. Extract financial data using a document parser or an LLM. This step isolates structured data such as revenue, expenses, net income, and growth metrics from unstructured text. You can use pattern-based parsing or prompt an LLM to extract only the specific values you care about, turning messy reports into clean, usable data.

  3. Summarize the extracted data using a language model. With the structured information in hand, the final stage generates a clear, human-readable summary that interprets the results. This summarization makes it easier for end-users, such as analysts or investors, to understand the key takeaways without needing to review the full reports.


Each stage depends on the previous one. If you reverse the order, the model won’t have the necessary context. Using tools like LangChain or Databricks notebooks, you can automate this sequencing to enforce the correct logic. 

###### Tip

Use SequentialChain or RouterChain in LangChain to control flow based on task type or input characteristics.

Here’s how you might implement the preceding in Python using LangChain:
    
    
    from langchain.prompts import PromptTemplate
    from langchain.llms import OpenAI
    from langchain.chains import LLMChain, SimpleSequentialChain
     
    # Step 1: Extract key financial terms
    extract_prompt = PromptTemplate.from_template(
        "Extract the key financial terms from this report: {input}"
    )
    extract_chain = LLMChain(llm=OpenAI(), prompt=extract_prompt)
     
    # Step 2: Summarize findings
    summary_prompt = PromptTemplate.from_template(
        "Summarize the implications of the following financial terms: {input}"
    )
    summary_chain = LLMChain(llm=OpenAI(), prompt=summary_prompt)
     
    # Chain the tools in order
    chain = SimpleSequentialChain(chains=[extract_chain, summary_chain], verbose=True)
     
    # Run the full process
    result = chain.run("The Q4 report showed increased revenue but lower net margins.")
    print(result)

This script demonstrates how `SimpleSequentialChain` executes one step after another, ensuring that the output from the first prompt is passed directly into the second. Each chain utilizes a shared input format, providing seamless integration. 

###### Tip

Use `SimpleSequentialChain` for straightforward, linear flows. For more control over variable names, branching, or error handling, consider `RunnableSequence` from `langchain_core`.

In this example:

  * The first prompt extracts financial terms from a document.

  * The second uses those terms to create an executive summary.

  * LangChain ensures that these steps are executed correctly and in sequence.


This process reflects real-world use cases in AI pipelines: _extract → reason → respond_ _**.**_ Whether you’re working in finance, healthcare, or customer service, multi-stage reasoning enables you to combine accuracy, traceability, and flexibility in your solutions.

## Planning Agent Chains

Some problems don’t follow a fixed, linear path. In real-world situations, such as troubleshooting or research, you often can’t predict how many steps are needed or which tools the system should use. That’s where _planning agents_ come in. Planning agents combine reasoning with tool selection to make dynamic decisions about what to do next, step by step.

A planning agent works like a smart assistant. It begins with a goal, considers the available tools as needed, and stops until you get a satisfactory answer. This process makes it ideal for tasks that involve branching logic, information lookups, or dynamic decision-making.

Take a telecom scenario, for instance. Imagine a network-monitoring AI assistant responsible for diagnosing service outages across regions. When triggered by a user prompt, such as _Find the cause of the outage in New York,_ the agent doesn’t follow a fixed set of rules. Instead, it evaluates the situation step by step based on available tools and evidence it uncovers.

It starts by examining recent outage logs. If it finds log entries that match the specified timeframe or region, it proceeds to the next step. The agent then queries a user complaint database to identify any corroborating reports from affected customers. If the cause remains unclear, such as if the logs are inconclusive, it checks external weather data for that location and time to determine if adverse conditions (like a storm or flooding) may have played a role. Finally, it assembles a root cause analysis in natural language and presents it back to the user after gathering enough supporting evidence.

This process is not hardcoded. The agent uses its reasoning capabilities to decide whether to proceed, skip, or combine steps depending on what information it retrieves. This flexibility is especially valuable in dynamic, real-time environments, such as telecom, where causes vary, and data availability can differ from case to case.

Rather than hardcoding this sequence, a planning agent decides which steps to take based on what it finds. This process allows it to adapt the flow dynamically, just like a human expert would. [Figure 2-3](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch02.html#ch02_figure_3_1755111237393669) shows the difference between sequential tool ordering and planning agent reasoning.

![A diagram of a tool and a tool

AI-generated content may be incorrect.](images/ch02_figure_3.png)

###### Figure 2-3. Sequential tool ordering vs. planning agent reasoning

## Agent Planning with LangChain

LangChain supports agent-style workflows using patterns like _ReAct_ (Reason + Act) and _Plan-and-Execute_. These patterns let the agent do the following:

  1. _Receive a natural language task_ _**:**_ The agent starts by interpreting the user’s request, such as _Find the cause of the outage in New York._ It transforms this into an internal goal that will guide its reasoning process.

  2. _Plan which tools to use and in what order_ : The agent reviews the available tools and considers which are most appropriate to solve the problem. It evaluates what kind of information it needs first (e.g., logs) and what it might need next (e.g., user reports or weather), and organizes a possible sequence of tool invocations.


_Execute those tools as needed_ _**:**_ Guided by its plan, the agent calls each tool step by step. After each execution, it reassesses the situation to determine whether it requires additional information or can generate a final response. This loop continues until the agent completes the task or determines no further action is required. The loop is as follows: Reason, act, plan, execute. You can run these agents in Databricks notebooks by integrating LangChain tools and logic. Let’s walk through a simple example using LangChain’s ReAct framework to build a planning agent:
    
    
    from langchain.agents import initialize_agent, Tool
    from langchain.llms import OpenAI
    from langchain.agents.agent_types import AgentType
     
    # Define tools the agent can use
    def search_logs(query):
        return f"Found matching log entry for: {query}"
     
    def check_weather(location):
        return f"Current weather in {location} is clear."
     
    tools = [
        Tool(
            name="SearchLogs",
            func=search_logs,
            description="Use this to search system logs for outages."
        ),
        Tool(
            name="WeatherChecker",
            func=check_weather,
            description="Use this to check the current weather conditions."
        )
    ]
     
    # Initialize the agent
    llm = OpenAI(temperature=0)
    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True
    )
     
    # Ask the agent to investigate an outage
    result = agent.run("Find the cause of the outage in New York region")
    print(result)

Let’s look at how this code works step by step:

  1. The agent receives the goal: investigate a New York outage.

  2. It analyzes the user’s prompt to understand the objective and identifies _New York_ as the target location and _outage_ as the core problem.

  3. It then evaluates which tools are most relevant. If system logs are available, the agent may begin by searching them for matching entries in the specified region.

  4. If it finds useful logs, it determines whether further investigation is needed by checking if user complaints corroborate the system data.

  5. When the evidence remains inconclusive, it proactively selects additional tools, such as checking real-time weather conditions, to identify environmental causes.

  6. After synthesizing all retrieved data, the agent forms a comprehensive understanding and generates a clear, context-aware explanation of the likely cause of the outage.

  7. It stops once the final output satisfies the original query or when further steps no longer add value.


This decision-making loop—observe, choose, execute, reassess—makes planning agents effective in adaptive, data-driven workflows. This mirrors how real-world agents operate in fields such as customer support, IT operations, or security triage.

###### Tip

Planning agents provide flexibility and adaptability. Use them for problems where you can’t define the path upfront. Agents add power but also introduce complexity. Monitor for edge cases, infinite loops, or misuse of the tool. Use logging and timeouts to maintain reliability.

Planning agents enable your LLM applications to transition from scripted flows to intelligent behavior. As you build advanced pipelines in Databricks, mastering this pattern will help you design scalable, robust, and context-aware solutions. Multi-stage reasoning enables better control, interpretability, and task decomposition in large language models (LLMs) workflows. You’ve learned how to implement both _sequential chains_ and _dynamic planning agents_ using LangChain in Databricks.

In the next section, you’ll go one step further and design _multi-agent systems_ , where multiple agents collaborate, specialize, and hand off tasks to achieve complex goals efficiently.

# Designing Multi-Agent Systems with Tool Use

As tasks grow more complex—such as end-to-end business processes, cross-domain decisions, or multi-step analytics—it becomes impractical to expect a single agent to manage all decisions, tools, and reasoning paths. That’s where _multi-agent systems_ become essential. In this architecture, multiple agents are assigned specialized roles, each equipped with specific tools and logic to handle a part of the overall task.

Designing multi-agent systems involves creating a collaborative ecosystem where agents can work independently yet interoperate seamlessly. For instance, in an AI-powered business workflow, you might assign one agent to gather financial reports, another to interpret them for risk exposure, and a third to generate recommendations. Each agent contributes a specific function, but collectively, they create a more intelligent and responsive pipeline. These agents pass context-rich data and decisions from one stage to the next, improving overall system performance, transparency, and scalability.

This process mirrors how humans work in teams—breaking large goals into smaller, role-specific tasks. In AI systems, multi-agent design facilitates the creation of reusable, modular, and adaptable workflows that can evolve without requiring a complete rebuild from scratch. Let’s understand this with an example:

## Scenario: Insurance Claim Processing

Consider an AI system designed to automate complex insurance claim workflows. Rather than relying on a single, all-purpose agent to handle every aspect of the claim lifecycle, it’s far more efficient to distribute responsibilities across multiple specialized agents, each with domain-specific tools and logic:

Document agent
    

This agent processes incoming claim documents, such as scanned PDFs or handwritten forms, extracting structured fields like claimant information, accident dates, and policy numbers. It ensures that noisy, unstructured text is converted into clean data.

Policy agent
    

After the document agent completes its task, the structured data is passed to the policy agent, which verifies the extracted details against the customer’s policy. It validates coverage, flags any policy violations or limitations, and confirms eligibility for further assessment.

Assessment agent
    

If coverage is valid, the data is passed to an assessment agent, which estimates the cost of repair or replacement. It may use internal rules, third-party APIs, or damage-assessment models trained on historical claim data.

Resolution agent
    

Once the assessment is done, the final resolution agent compiles all prior outputs and generates a natural language decision summary, such as an approval letter, payment estimate, or denial explanation.

These agents operate as a coordinated system. They pass data downstream but may also send it backward for revision if inconsistencies or missing information are detected, just like how a human team might ask someone to double-check the inputs before proceeding.

Here’s a simplified illustration of how agents might interact in a coordinated system:
    
    
    from langchain.agents import Tool, initialize_agent, AgentType
    from langchain.llms import OpenAI
     
    # Define basic tools each agent might use
    def extract_claim_details(text):
        return "Claimant: John Doe, Damage: $1500, Policy#: 789XYZ"
     
    def validate_policy(policy_id):
        return f"Policy {policy_id} is valid and active."
     
    def assess_damage(details):
        return "Estimated repair cost: $1,500"
     
    def finalize_resolution(assessment):
        return "Claim approved. Notification sent."
     
    # Register as tools for agent use
    tools = [
        Tool(name="ExtractClaim", func=extract_claim_details, description="Extract data from claim documents."),
        Tool(name="CheckPolicy", func=validate_policy, description="Validate policy coverage."),
        Tool(name="DamageAssessment", func=assess_damage, description="Assess damage value."),
        Tool(name="ResolutionGenerator", func=finalize_resolution, description="Create final claim decision message.")
    ]
     
    # Set up controller agent
    llm = OpenAI(temperature=0)
    agent = initialize_agent(tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
     
    # Run a coordinated workflow
    result = agent.run("Process an insurance claim for policy ID 789XYZ with reported vehicle damage.")
    print(result)

This example simulates a multi-agent architecture where each step uses a specific tool, and a controller coordinates execution. This type of system reduces cognitive load per agent, increases transparency, and allows you to update one agent’s logic (e.g., pricing models) without changing the rest of the pipeline. [Figure 2-4](https://learning.oreilly.com/library/view/databricks-certified-generative/9798341623446/ch02.html#ch02_figure_4_1755111237393686) shows this process.

![A diagram of a diagram

AI-generated content may be incorrect.](images/ch02_figure_4.png)

###### Figure 2-4. Insurance claim multi-agent architecture

[Note]**** This inter-agent communication requires shared memory, clear task boundaries, and a consistent data format. You can manage this using orchestrators like LangChain’s AgentExecutor or shared context managers in Databricks notebooks.

## Key Design Principles of Multi-Agent Systems

To build effective multi-agent systems, you need to apply principles that promote clarity in responsibilities, modularity in execution, and resilience in decision-making. These principles serve as a foundation to ensure each agent operates independently yet harmoniously within a larger system:

### Define agent roles clearly

Every agent should have a well-defined and scoped responsibility. For example, one agent handles data extraction while another manages compliance checking. This modularity makes systems easier to maintain and allows for targeted improvements or debugging.

However, if you don’t define agent roles clearly, overlapping responsibilities can lead to conflicts, redundant work, or data inconsistencies. For instance, two agents trying to process and validate the same document independently might return contradictory results or overwrite each other’s outputs. It also becomes harder to identify where errors originate or how to isolate underperforming components. A lack of clear role boundaries renders the system a tangle of loosely coordinated behaviors rather than a streamlined, cooperative architecture.

### Use consistent interfaces

All agents should communicate through predictable formats, such as structured JSON or validated schemas. This predictable format reduces integration errors, prevents misinterpretation between agents, and enables plug-and-play design when swapping or upgrading agents.

For example, imagine an assessment agent outputs damage estimates in a nested JSON structure, but the resolution agent expects a flat CSV-like dictionary. If these formats aren’t aligned, the system could either crash or silently generate incorrect summaries. In contrast, using a shared schema, validated by both agents, ensures data integrity and consistent understanding.

Without consistent interfaces, agents may misread or fail to process each other’s outputs correctly. These inconsistencies can result in dropped data, incorrect assumptions, or redundant error-handling logic. The lack of standardization breaks the chain of collaboration and makes debugging across agent boundaries extremely difficult.

### Support asynchronous or iterative reasoning

The agent should be designed to operate independently, where possible. This process enables parallel processing—for instance, running a fraud detection agent and a policy validation agent simultaneously. In some cases, agents may need to trigger one another recursively to refine results through iterative passes.

For example, in a financial workflow, a compliance agent and a risk evaluation agent can operate on the same transaction simultaneously. The compliance agent checks regulatory thresholds while the risk agent assesses exposure based on historical data. If either agent flags an issue, it could trigger a review agent that reruns earlier evaluations with stricter parameters. This not only speeds up the process but adds robustness through parallel verification.

Without support for asynchronous or iterative reasoning, agents are forced into a rigid sequence, even when steps could safely run in parallel. This leads to longer runtimes, underutilized resources, and difficulty scaling workflows for high-volume or real-time systems.

### Use centralized coordination if needed

In more complex flows, use a controller agent or orchestrator to manage task routing, execution order, and dependency resolution. This coordination layer is essential when the workflow includes conditional logic, dependencies between agents, or real-time decisions that depend on multiple inputs.

For example, consider a multi-agent retail pricing system. One agent monitors competitor pricing, another checks inventory levels, and a third recommends discounts. Without a coordinator, these agents might operate out of sync, leading to a situation where the pricing agent drops a price below cost without knowing inventory is already running low. A centralized coordinator ensures that actions are sequenced logically, constraints are respected, and insights from one agent inform the others in real-time.

When coordination is missing, agents may duplicate work, contradict each other, or miss dependencies. This results in erratic outcomes, a poor user experience, and increased maintenance complexity. A well-structured orchestrator prevents these issues by enforcing order and maintaining context across the full system.

By applying these principles, your multi-agent system becomes more scalable, interpretable, and resilient to faults. If one component underperforms or requires updates, you can address it independently without disrupting the entire system.

This structure enables your AI workflows to be more scalable, interpretable, and resilient to faults. If one agent fails or underperforms, you can isolate and refine it without having to rewrite the entire system.

# Summary

In this chapter, you explored how to design effective Generative AI applications by aligning prompts, tasks, and workflows with real business needs. You learned how to craft structured prompts, match model tasks like classification or extraction to use cases, and build modular pipelines that separate responsibilities. Most importantly, you saw how multi-stage reasoning, through tool ordering and planning agents, helps break down complex workflows into manageable, repeatable steps.

Applying these techniques in tools like LangChain or Databricks notebooks allows you to build smarter, more reliable AI solutions that adapt to real-world data and decisions.

The next chapter dives deeper into how to prepare data for LLMs and RAG.

# Practice Questions

Following are multiple-choice questions to reinforce your understanding of the concepts in this chapter. These are structured to reflect the style and expectations of the Databricks Generative AI Associate certification exam. The answer key appears at the end of the questions.

  1. **1.** What is the main purpose of using a `PromptTemplate` in LangChain?


A. To speed up the LLM inference process

B. To control the token length in the LLM output

C. To define reusable, parameterized prompts with a consistent structure

D. To deploy models to a REST API

  1. **2.** Which of the following use cases requires the _extraction_ task type?


A. Generating a summary of a quarterly report

B. Classifying customer emails into support categories

C. Pulling fields like revenue and expenses from a financial document.

D. Rewriting a legal clause into plain English

  1. **3.** What is the benefit of using a few-shot example in structured prompting?


A. It reduces latency in prompt execution

B. It teaches the model the desired output behavior using in-context examples

C. It eliminates the need for downstream output parsing

D. It allows the model to avoid schema definitions

  1. **4.** In a multi-stage reasoning system, why is task decomposition critical?


A. It improves randomness in the generated output

B. It allows the system to skip all validation steps

C. It isolates each reasoning step for better accuracy and traceability

D. It reduces the number of tools needed in LangChain

  1. **5.** In LangChain, what does `SimpleSequentialChain` enable you to do?


A. Execute prompts in parallel for performance

B. Run tools with dynamic ordering based on input

C. Enforce fixed step-by-step execution using prompt chains

D. Automatically validate schema compliance

  1. **6.** What happens if you choose the wrong LLM task type for your use case?


A. The system defaults to classification

B. You may receive unstructured or irrelevant outputs

C. The prompt will be ignored by the model

D. LangChain will automatically correct the task type

  1. **7.** Which principle is _not_ part of effective multi-agent system design?


A. Shared memory between agents

B. Role clarity for each agent

C. Randomized tool execution

D. Standardized interfaces

  1. **8.** What does the agent pattern ReAct enable within LangChain?


A. Real-time context memory storage

B. reasoning and tool calling in a step-by-step loop

C. Visualization of prompt chains

D. Summarization of multi-agent outputs

  1. **9.** Why are prompt templates called the “API contract” for LLMs?


A. They enable testing through RESTful endpoints

B. They define static output schemas only

C. They allow consistent and expected interaction with the LLM

D. They store access credentials for model endpoints

  1. **10.** You’re asked to extract _claim_type_ and _urgency_ from unstructured insurance text. Which task and prompt style should you use?


A. Text generation with an open-ended prompt

B. Classification with a multiple-choice prompt

C. Extraction with a schema-defined prompt

D. Transformation using a plain-language template

Table 2-5. Answer Key:Question| Answer| Explanation  
---|---|---  
1|  **C** | `PromptTemplate` enables reusable, parameterized prompts with placeholders for consistent structure.  
2|  **C** | Extraction is used to extract structured fields, such as revenue and expenses, from unstructured text.  
3|  **B** | Few-shot examples guide the model to produce consistent outputs by showing desired behavior.  
4|  **C** | Task decomposition improves accuracy and traceability by isolating logical steps.  
5|  **C** | `SimpleSequentialChain` runs chains step-by-step in a fixed order, passing output to the next stage.  
6|  **B** | Misaligned task types result in outputs that are either irrelevant or unusable, such as prose instead of structured data.  
7|  **C** | Randomized tool execution would lead to inconsistent or chaotic agent behavior, ReActthereby violating system integrity.  
8|  **B** | ReAct agents “reason” and “act” in a loop, selecting and using tools step-by-step.  
9|  **C** | Prompt templates define the structure and expectations, much like an API contract defines Input/output (I/O).  
10|  **C** | Extraction with schema guidance (e.g., via JSON keys) is the correct pattern for structured data from free text.  
  
# Hands-On Lab: Multi-Agent Workflow with LangChain + OpenAI in Databricks

## Scenario

You’re working with a company that handles auto insurance claims. Rather than using one large AI model to handle all tasks, you’ll create a modular pipeline composed of four specialized agents, each designed for a specific step in the claim process:

Document Agent
    

Extracts structured data from a claim report stored in a Databricks table.

Policy Agent
    

Validates whether the associated policy is active and eligible.

Assessment Agent
    

Uses an LLM to determine if the claim should be auto-approved or flagged for manual review.

Resolution Agent
    

Generates a final decision statement based on the previous step.

This agent-based pipeline approach reflects real-world enterprise deployments where interpretability, traceability, and modularity are essential.

## Objective

By the end of this lab, you will:

  * Understand how to configure LangChain tools in Databricks and chain multiple specialized tools using LangChain agents

  * Create reusable functions and prompts for each step of the workflow

  * Use LangChain’s agent framework to coordinate tasks in a zero-shot fashion

  * Observe how each agent contributes to the overall decision-making process and coordinate decisions like validation, risk assessment, and resolution

  * Use OpenAI to reason through structured task steps


You will simulate the entire pipeline inside a Databricks notebook using PySpark for data setup and OpenAI for decision logic. This lab provides a practical foundation for building multi-step intelligent workflows using LLMs and agent orchestration tools.

###### Note

You can access this lab on Github [here](https://github.com/rkaushik2007/Databricks-Certified-Generative-AI-Engineer-Associate-Study-Guide/tree/main/Chapter%202).

table of contents

search

Settings
