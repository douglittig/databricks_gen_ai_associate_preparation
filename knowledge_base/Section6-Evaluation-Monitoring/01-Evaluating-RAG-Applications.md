# Evaluating a RAG Application and Continual Learning

## Slide 1: Evaluating a RAG Application and Continual Learning
In this lecture Evaluating a RAG Application and Continual Learning, we cover evaluating the RAG pipeline, key evaluation metrics such as context precision, relevancy, recall, faithfulness, answer quality, and the use of MLflow for LLM evaluation.

## Slide 2: Evaluating RAG Pipeline - RAG is complex
Let’s dive into how you evaluate a retrieval augmented generation (RAG) application. Since a RAG app involves several components, assessment means looking at each part and its strategies or hyperparameters. This includes checking if chunking is working, whether the token size or overlap is optimal, and if a different splitter might help. You need to ask if your embedding model is good enough or if you should fine-tune or switch models. It’s also important to evaluate the retrieval quality, the performance of any re-ranker, and how fast the process runs. Generally, generator-level issues are less common; most problems arise at the retrieval stage and are often caused by poor data preparation. So, always remember the principle: garbage in, garbage out.

## Slide 3: Evaluation Metrics - Retrieval and generation related metrics
To run evaluations for a RAG application, you need several key pieces of information: the query (which is the original user question), the context (retrieved from the vector store), and the model’s generated response. In addition, if you have access to ground truth—what would be considered a correct response or an expected set of retrieved documents—that can further improve your evaluation. The core questions you’re aiming to answer are: is the context relevant to the query, is the response genuinely using the provided context, and is the final output both relevant to the original question and consistent with the ground truth? Sometimes, the LLM might ignore the context and rely only on its own training, which could indicate the need for better prompting or issues like the information being hard to find in the retrieved context. Ultimately, assessing relevance both to the query and to the ground truth is essential.

## Slide 4: Context Precision - Retrieval related metrics
Here’s a look at one important evaluation metric: context precision. This measures how much of the retrieved content directly relates to the query—think of it like the signal-to-noise ratio for the retrieved context. High context precision means the context is closely tied to the query; for example, if you ask about Einstein's role in quantum mechanics and the retrieved context specifically discusses his contributions, the precision is high. Lower scores occur when the context contains irrelevant information—for instance, details about Einstein's personal life instead of his work on quantum theory. Essentially, context precision helps you judge whether the top retrieved chunks or document nodes are truly relevant and focused in relation to the original query.

## Slide 5: Context Relevancy - Retrieval related metrics
Context relevance is a metric used to assess how well the retrieved context relates to the original query. It doesn’t measure factual accuracy but rather checks if the content answers the posed question. For example, with a query about Einstein’s role in quantum mechanics, high context relevancy would mean the context includes details about his skepticism and subsequent foundational contributions to quantum theory. In contrast, low relevancy would be content about Einstein’s personal life or achievements that have nothing to do with quantum mechanics. The metric helps determine if the context truly addresses the user’s question.

## Slide 6: Context Recall - Retrieval related metrics
Context recall is a metric used to compare the retrieved context with a known ground truth. If your evaluation dataset identifies which sources or specific paragraphs should be retrieved for a query, context recall measures how well the system brings back these expected pieces. For example, if the question is about which scientific theories Einstein contributed to, the ground truth would include both relativity and quantum mechanics. If the retrieved context covers both, you get a high context recall score. If it mentions only one or brings up unrelated achievements, the recall is lower. This metric is valuable for checking how completely the retrieval covers the expected, relevant information.

## Slide 7: Faithfulness - Generation related metrics
Faithfulness measures how well the generated response from the language model sticks to the retrieved context, focusing on the generation side rather than retrieval. If the LLM ignores the provided information—perhaps due to too much context, overlooked chunks, poor prompting, or its own static training data—the answer may be inaccurate or hallucinated. For example, if you ask about Einstein’s birth date and the retrieved context states March 14th, but the answer provides a different date, that’s a sign of low faithfulness. The solution could be clearer prompts or better context chunking to keep the answer tied to the relevant facts and avoid truncation due to token limits.

## Slide 8: Answer Relevancy - Generation related metrics
Answer relevancy is a global metric used to judge how well the generated response aligns with the original user query, similar to what you’d assess outside of RAG systems. It checks the pertinence and applicability of the answer to the user's intent. For instance, if the user asks what Einstein is known for, a highly relevant answer would mention the theory of relativity; a low relevancy answer might simply state he was a scientist, which is too vague. Frameworks can often calculate metrics like answer relevancy, provided you have queries, responses, retrieved context, and ground truth data in your evaluation set. While it’s common to always have the query and response, obtaining accurate ground truths and context can sometimes be challenging, especially if logging is incomplete during chain construction. Ultimately, capturing enough evaluation data is key for reliably assessing answer relevancy and other important metrics.

## Slide 9: Answer Correctness - Generation related metrics
Answer correctness is a metric that compares the generated response with the expected ground truth, similar to comparing predictions to labels in traditional ML workflows. It takes into account both semantic and factual similarity. For instance, if the query is about why and when Einstein won the Nobel Prize in Physics, a highly correct answer would state he won it in 1921 for his work on the photoelectric effect. An answer that lists the wrong year or credits him for the theory of relativity instead would score low on correctness. This metric is useful for ensuring both factual and conceptual accuracy in responses.

## Slide 10: MLflow (LLM) Evaluation - Efficiently evaluate retrievers and LLMs
All these RAG metrics—like precision, relevancy, recall, faithfulness, and answer correctness—can be implemented out of the box using MLflow’s LLM evaluation suite. This toolset allows you to apply standard, generic metrics but also supports adding custom metrics tailored to your unique requirements, ensuring that “correctness” reflects what’s important for your specific use case. MLflow’s evaluation suite lets you benchmark open-source and proprietary models, compare different foundation models, and can even help reduce the need for manual human evaluation. However, human involvement remains vital, especially for creating ground truth or labeled data. The flexibility to combine standard and custom metrics gives you robust control over evaluating and improving your RAG system.

## Slide 11: MLflow (LLM) Evaluation - Efficiently evaluate retrievers and LLMs (Interactive)
The aim is to reduce human intervention in evaluation since it can be expensive and time-consuming. This is achieved by using batch offline evaluation, where you test your chain on an evaluation dataset before moving it to staging or production. A popular, efficient approach is to use another language model—LLM as a judge—to automatically assess outputs, a method both cost-effective and quick to set up. Databricks promotes and supports this practice through the MLflow Evaluate API. Additionally, the MLflow tracking server’s UI makes it easy to create new prompts, test queries, and compare model completions across different models or inputs, further streamlining evaluation and model selection with minimal manual effort.